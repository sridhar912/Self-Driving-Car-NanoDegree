{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self-Driving Car Engineer Nanodegree\n",
    "\n",
    "## Deep Learning\n",
    "\n",
    "## Project: Build a Traffic Sign Recognition Classifier\n",
    "\n",
    "In this notebook, a template is provided for you to implement your functionality in stages which is required to successfully complete this project. If additional code is required that cannot be included in the notebook, be sure that the Python code is successfully imported and included in your submission, if necessary. Sections that begin with **'Implementation'** in the header indicate where you should begin your implementation for your project. Note that some sections of implementation are optional, and will be marked with **'Optional'** in the header.\n",
    "\n",
    "In addition to implementing code, there will be questions that you must answer which relate to the project and your implementation. Each section where you will answer a question is preceded by a **'Question'** header. Carefully read each question and provide thorough answers in the following text boxes that begin with **'Answer:'**. Your project submission will be evaluated based on your answers to each of the questions and the implementation you provide.\n",
    "\n",
    ">**Note:** Code and Markdown cells can be executed using the **Shift + Enter** keyboard shortcut. In addition, Markdown cells can be edited by typically double-clicking the cell to enter edit mode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 1: Dataset Exploration\n",
    "\n",
    "Visualize the German Traffic Signs Dataset. This is open ended, some suggestions include: plotting traffic signs images, plotting the count of each sign, etc. Be creative!\n",
    "\n",
    "\n",
    "The pickled data is a dictionary with 4 key/value pairs:\n",
    "\n",
    "- features -> the images pixel values, (width, height, channels)\n",
    "- labels -> the label of the traffic sign\n",
    "- sizes -> the original width and height of the image, (width, height)\n",
    "- coords -> coordinates of a bounding box around the sign in the image, (x1, y1, x2, y2). Based the original image (not the resized version)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted training data..\n",
      "Extracted test data..\n"
     ]
    }
   ],
   "source": [
    "# Load pickled data\n",
    "import pickle\n",
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline                 \n",
    "\n",
    "# TODO: fill this in based on where you saved the training and testing data\n",
    "training_file = 'lab 2 data/train.p'\n",
    "testing_file = 'lab 2 data/test.p'\n",
    "\n",
    "with open(training_file, mode='rb') as f:\n",
    "    train = pickle.load(f)\n",
    "with open(testing_file, mode='rb') as f:\n",
    "    test = pickle.load(f)\n",
    "    \n",
    "X_train, y_train = train['features'], train['labels']\n",
    "print('Extracted training data..')\n",
    "X_test, y_test = test['features'], test['labels']\n",
    "print('Extracted test data..')\n",
    "\n",
    "is_data_read = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples = 39209\n",
      "Number of testing examples = 12630\n",
      "Image data shape = (32, 32, 3)\n",
      "Number of classes = 43\n"
     ]
    }
   ],
   "source": [
    "### To start off let's do a basic data summary.\n",
    "\n",
    "assert is_data_read,'You failed to load the data'\n",
    "\n",
    "# TODO: number of training examples\n",
    "n_train = X_train.shape[0]\n",
    "\n",
    "# TODO: number of testing examples\n",
    "n_test = X_test.shape[0]\n",
    "\n",
    "# TODO: what's the shape of an image?\n",
    "image_shape = X_train.shape[1:]\n",
    "\n",
    "# TODO: how many classes are in the dataset\n",
    "#le = preprocessing.LabelEncoder()\n",
    "#le.fit(y_train)\n",
    "#n_classes = le.classes_.shape[0]\n",
    "#n_classes = len(set(y_train))\n",
    "n_classes = len(np.unique(y_train))\n",
    "\n",
    "print(\"Number of training examples =\", n_train)\n",
    "print(\"Number of testing examples =\", n_test)\n",
    "print(\"Image data shape =\", image_shape)\n",
    "print(\"Number of classes =\", n_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "## Step 2: Design and Test a Model Architecture\n",
    "\n",
    "Design and implement a deep learning model that learns to recognize traffic signs. Train and test your model on the [German Traffic Sign Dataset](http://benchmark.ini.rub.de/?section=gtsrb&subsection=dataset).\n",
    "\n",
    "There are various aspects to consider when thinking about this problem:\n",
    "\n",
    "- Your model can be derived from a deep feedforward net or a deep convolutional network.\n",
    "- Play around preprocessing techniques (normalization, rgb to grayscale, etc)\n",
    "- Number of examples per label (some have more than others).\n",
    "- Generate fake data.\n",
    "\n",
    "Here is an example of a [published baseline model on this problem](http://yann.lecun.com/exdb/publis/pdf/sermanet-ijcnn-11.pdf). It's not required to be familiar with the approach used in the paper but, it's good practice to try to read papers like these."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation\n",
    "\n",
    "Use the code cell (or multiple code cells, if necessary) to implement the first step of your project. Once you have completed your implementation and are satisfied with the results, be sure to thoroughly answer the questions that follow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image preprocessing is successful..\n",
      "(9, 32, 32)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWEAAAFfCAYAAACfj30KAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fe8243c4048>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgcAAAFkCAYAAAC0KZhSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nDiRJUsZwIEmSMoYDSZKU+f8BF4H7vIbocQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fe821d82940>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### Preprocess the data here.\n",
    "# Normalize all images so that value are between -1 to 1\n",
    "import cv2\n",
    "from sklearn.preprocessing import scale, StandardScaler\n",
    "image_value_ranges = [0.1,0.9]\n",
    "\n",
    "def convert_rgb2yuv(image_data):\n",
    "    yuv_image_data = []\n",
    "    if len(image_data.shape) > 3: \n",
    "        for i in range(len(image_data)):\n",
    "            yuv_image_data.append(cv2.cvtColor(image_data[i], cv2.COLOR_RGB2YUV))\n",
    "    else:\n",
    "        yuv_image_data.append(cv2.cvtColor(image_data, cv2.COLOR_RGB2YUV))\n",
    "    return np.array(yuv_image_data)\n",
    "\n",
    "def convert_rgb2gray(image_data):\n",
    "    gray_image_data = []\n",
    "    for i in range(len(image_data)):\n",
    "        gray_image_data.append(cv2.cvtColor(image_data[i], cv2.COLOR_RGB2GRAY))\n",
    "    return np.array(gray_image_data)\n",
    "\n",
    "def normalize_Y_lecun(image_data):\n",
    "    im_data = np.array(image_data,np.float32)\n",
    "    #im_reshape = np.reshape(image_data,(-1,1024))\n",
    "    #standard_scaler = StandardScaler(with_mean=True)\n",
    "    #im_rescale = standard_scaler.fit_transform(im_reshape)\n",
    "    #im_rescale = scale(im_reshape, axis=0, with_mean=True, with_std=False)\n",
    "    #im_data = np.reshape(im_rescale,(-1,32,32))\n",
    "    minV = np.amin(im_data)\n",
    "    maxV = np.amax(im_data)\n",
    "    lowerLimit = image_value_ranges[0]\n",
    "    upperLimit = image_value_ranges[1]\n",
    "    im_data =  lowerLimit + ((im_data - minV)*(upperLimit - lowerLimit))/(maxV - minV)\n",
    "    return im_data\n",
    "\n",
    "def normalize_Y(image_data,sub_mean = False,use_channel=False):\n",
    "    \"\"\"\n",
    "    Normalize the image data with Min-Max scaling to a range of [0.1, 0.9]\n",
    "    :param image_data: The image data to be normalized\n",
    "    :return: Normalized image data\n",
    "    \"\"\"\n",
    "    # ToDo: Implement Min-Max scaling for greyscale image data\n",
    "    minV = np.amin(image_data)\n",
    "    maxV = np.amax(image_data)\n",
    "    #minV = 0\n",
    "    #maxV = 255\n",
    "    lowerLimit = image_value_ranges[0]\n",
    "    upperLimit = image_value_ranges[1]\n",
    "    image_data = np.array(image_data,np.float32)\n",
    "    image_data[:,:,:,0] =  lowerLimit + ((image_data[:,:,:,0] - minV)*(upperLimit - lowerLimit))/(maxV - minV)\n",
    "    #image_data =  lowerLimit + ((image_data - minV)*(upperLimit - lowerLimit))/(maxV - minV)\n",
    "    if sub_mean:\n",
    "        image_data[:,:,:,0] = image_data[:,:,:,0] - np.mean(image_data[:,:,:,0], axis=0)\n",
    "    if use_channel:\n",
    "        return image_data\n",
    "    else:\n",
    "        return image_data[:,:,:,0]\n",
    "\n",
    "def preprocess_images(image_data, use_only_y = True, use_mean = False):\n",
    "    # Convert rgb color format to yuv format \n",
    "    p_image_data = convert_rgb2yuv(image_data)\n",
    "    p_image_data = normalize_Y_lecun(p_image_data[:,:,:,0])\n",
    "    \n",
    "    #if use_only_y:\n",
    "        #p_image_data = normalize_Y(p_image_data,use_channel=False)\n",
    "    #else:\n",
    "        #p_image_data = normalize_Y(p_image_data,use_channel=True)\n",
    "\n",
    "    return p_image_data\n",
    "\n",
    "def pre_process_images_test(image_data):\n",
    "    p_image_data = convert_rgb2yuv(image_data)\n",
    "    \n",
    "\n",
    "print('Image preprocessing is successful..')\n",
    "is_preprocess_defined = True\n",
    "\n",
    "# Testing\n",
    "samples = X_train[1:10] \n",
    "slabels = y_train[1:10]\n",
    "norm_sample = preprocess_images(samples)\n",
    "#print(t_mean.shape)\n",
    "print(norm_sample.shape)\n",
    "plt.figure()\n",
    "plt.imshow(norm_sample[0],cmap='gray')\n",
    "plt.figure()\n",
    "_ = plt.hist(norm_sample[0].ravel(),bins=256, color='black')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1 \n",
    "\n",
    "_Describe the techniques used to preprocess the data._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Generate data additional (if you want to!)\n",
    "### and split the data into training/validation/testing sets here.\n",
    "### Feel free to use as many code cells as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "assert is_preprocess_defined,'Preprocessing image functions not defined'\n",
    "\n",
    "import skimage.transform as skimage_tf\n",
    "import skimage.exposure as exposure\n",
    "\n",
    "def shift_image_location(image, xoffset, yoffset):\n",
    "    rows,cols, depth = image.shape\n",
    "    tparam = skimage_tf.AffineTransform(translation = (xoffset,yoffset))\n",
    "    out = skimage_tf.warp(image,tparam)\n",
    "    assert((out.shape[0] == 32) & (out.shape[1] == 32))\n",
    "    # This conversion is required as OpenCV rgb2yuv does not accept float64\n",
    "    return out.astype(np.float32)\n",
    "    \n",
    "# function to rotate images by given degrees\n",
    "def rotate_image(image, degree):\n",
    "    rows, cols, depth = image.shape\n",
    "    rad = (np.pi / 180) * degree\n",
    "    tparam = skimage_tf.AffineTransform(rotation = rad)\n",
    "    out = skimage_tf.warp(image,tparam)\n",
    "    assert((out.shape[0] == 32) & (out.shape[1] == 32))\n",
    "    return out.astype(np.float32)\n",
    "    \n",
    "# function to resize the image\n",
    "def scale_image(image, ratio):\n",
    "    rows, cols, depth = image.shape\n",
    "    scale = skimage_tf.rescale(image,ratio)\n",
    "    m_rows, m_cols, m_depth = scale.shape\n",
    "    #print(ratio)\n",
    "    #print(scale.shape)\n",
    "    if ratio > 1.0:\n",
    "        #print('GT')\n",
    "        offset = m_rows - rows\n",
    "        out = scale[offset:offset+rows, offset:offset+cols]\n",
    "    else:\n",
    "        #print('LT')\n",
    "        out = np.zeros((rows,cols,depth))\n",
    "        offset = rows - m_rows\n",
    "        out[offset:offset+rows, offset:offset+cols] = scale\n",
    "    \n",
    "    assert((out.shape[0] == 32) & (out.shape[1] == 32))\n",
    "    return out.astype(np.float32)\n",
    "\n",
    "def affine_image(image, xoffset, yoffset, degree, ratio):\n",
    "    out = shift_image_location(image, xoffset, yoffset)\n",
    "    out = rotate_image(out, degree)\n",
    "    out = scale_image(out,ratio)\n",
    "    return out.astype(np.float32)\n",
    "\n",
    "def combined_operations(image, xoffset, yoffset, degree, ratio, choice):\n",
    "    out = shift_image_location(image, xoffset, yoffset)\n",
    "    out = rotate_image(out, degree)\n",
    "    out = scale_image(out,ratio)\n",
    "    out = change_intensity(out, choice)\n",
    "    return out.astype(np.float32)\n",
    "\n",
    "def change_intensity(image, choice):\n",
    "    rows, cols, depth = image.shape\n",
    "    if choice == 1:\n",
    "        rnd = 2 * np.random.random()\n",
    "        out = exposure.adjust_gamma(image,gamma=rnd)\n",
    "    elif choice == 2:\n",
    "        out = exposure.adjust_log(image)\n",
    "    else:\n",
    "        out = exposure.adjust_sigmoid(image)\n",
    "\n",
    "    assert((out.shape[0] == 32) & (out.shape[1] == 32))\n",
    "    return out.astype(np.float32)\n",
    "\n",
    "def jitter_image_data(input_images,input_labels,batch_size):\n",
    "    num_images = input_images.shape[0]\n",
    "    jitter_images = []\n",
    "    jitter_images_labels = [] \n",
    "    indx = np.random.choice(input_images.shape[0],batch_size,replace = False)\n",
    "    images = input_images[indx]\n",
    "    labels = input_labels[indx]\n",
    "    for imageIdx in range(batch_size):\n",
    "        xoffset = int(4 * np.random.random() - 2)\n",
    "        yoffset = int(4 * np.random.random() - 2)\n",
    "        degree = int (20 * np.random.random() - 10)\n",
    "        ratio = 0.2 * np.random.random() + 0.9\n",
    "        choice = np.random.randint(4)\n",
    "        # Affine\n",
    "        jitter_images.append(combined_operations(images[imageIdx], xoffset, yoffset, degree, ratio,choice))\n",
    "        jitter_images_labels.append(labels[imageIdx])\n",
    "    \n",
    "    return preprocess_images(np.array(jitter_images)), np.array(jitter_images_labels)\n",
    "\n",
    "\n",
    "def jitter_image_data_old(input_images,input_labels,batch_size):\n",
    "    num_images = input_images.shape[0]\n",
    "    jitter_images = []\n",
    "    jitter_images_labels = [] \n",
    "    indx = np.random.choice(input_images.shape[0],batch_size,replace = False)\n",
    "    images = input_images[indx]\n",
    "    labels = input_labels[indx]\n",
    "    for imageIdx in range(batch_size):\n",
    "            xoffset = int(4 * np.random.random() - 2)\n",
    "            yoffset = int(4 * np.random.random() - 2)\n",
    "            degree = int (20 * np.random.random() - 10)\n",
    "            ratio = 0.2 * np.random.random() + 0.9\n",
    "            choice = np.random.randint(4)\n",
    "            # Add original image to the jitter data\n",
    "            jitter_images.append(images[imageIdx])\n",
    "            jitter_images_labels.append(labels[imageIdx])\n",
    "            # Shift image\n",
    "            jitter_images.append(shift_image_location(images[imageIdx], xoffset, yoffset))\n",
    "            jitter_images_labels.append(labels[imageIdx])\n",
    "            # Rotate image\n",
    "            jitter_images.append(rotate_image(images[imageIdx], degree))\n",
    "            jitter_images_labels.append(labels[imageIdx])\n",
    "            # Scale image\n",
    "            jitter_images.append(scale_image(images[imageIdx], ratio))\n",
    "            jitter_images_labels.append(labels[imageIdx])\n",
    "            # Affine\n",
    "            jitter_images.append(affine_image(images[imageIdx], xoffset, yoffset, degree, ratio))\n",
    "            jitter_images_labels.append(labels[imageIdx])\n",
    "            # Brightness\n",
    "            jitter_images.append(change_intensity(images[imageIdx], choice))\n",
    "            jitter_images_labels.append(labels[imageIdx])\n",
    "    \n",
    "    s_ind = np.random.choice(input_images.shape[0],batch_size)\n",
    "    \n",
    "    assert(len(s_ind) == batch_size)\n",
    "    \n",
    "    return preprocess_images(np.array(jitter_images)[s_ind]), np.array(jitter_images_labels)[s_ind]\n",
    "\n",
    "# Testing\n",
    "#samples = X_train[1:100] \n",
    "#slabels = y_train[1:100]\n",
    "#out1, out2 = jitter_image_data(samples,slabels,32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels One-Hot Encoded\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Turn labels into numbers and apply One-Hot Encoding\n",
    "encoder = LabelBinarizer()\n",
    "encoder.fit(y_train)\n",
    "train_labels = encoder.transform(y_train)\n",
    "test_labels = encoder.transform(y_test)\n",
    "\n",
    "# Change to float32, so it can be multiplied against the features in TensorFlow, which are float32\n",
    "train_labels = train_labels.astype(np.float32)\n",
    "test_labels = test_labels.astype(np.float32)\n",
    "\n",
    "print('Labels One-Hot Encoded')\n",
    "is_labels_encod = True\n",
    "is_train_test_split = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training features and labels randomized and split.\n",
      "Number of training images 37248\n",
      "Number of validation images 1961\n",
      "Number of testing images 12630\n"
     ]
    }
   ],
   "source": [
    "assert is_preprocess_defined, 'You skipped the step to preprocess the images'\n",
    "assert is_labels_encod, 'You skipped the step to One-Hot Encode the labels'\n",
    "\n",
    "if not is_train_test_split:\n",
    "    # Get randomized datasets for training and validation\n",
    "    train_features, valid_features, train_labels, valid_labels = train_test_split(\n",
    "        X_train,\n",
    "        train_labels,\n",
    "        test_size=0.05,\n",
    "        random_state=832289)\n",
    "\n",
    "    #test_features = preprocess_images(X_test)\n",
    "\n",
    "    print('Training features and labels randomized and split.')\n",
    "    print('Number of training images {}'.format(train_features.shape[0]))\n",
    "    print('Number of validation images {}'.format(valid_features.shape[0]))\n",
    "    print('Number of testing images {}'.format(X_test.shape[0]))\n",
    "\n",
    "is_train_test_split = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2\n",
    "\n",
    "_Describe how you set up the training, validation and testing data for your model. If you generated additional data, why?_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# set up tensorflow\n",
    "import tensorflow as tf\n",
    "\n",
    "#Placeholder definition\n",
    "features = tf.placeholder(tf.float32, [None, 32, 32])\n",
    "image = tf.reshape(features, [-1,32,32,1])\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "labels = tf.placeholder(tf.float32, [None, 43])\n",
    "\n",
    "def weight_variable(shape):\n",
    "  initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "  return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "  initial = tf.constant(0.1, shape=shape)\n",
    "  return tf.Variable(initial)\n",
    "\n",
    "def conv2d(x, W):\n",
    "  return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "  return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n",
    "                        strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "def model_twolayers():\n",
    "    endpoints = {}\n",
    "    with tf.variable_scope('conv1') as scope:\n",
    "        W_conv1 = weight_variable([5, 5, 1, 32])\n",
    "        b_conv1 = bias_variable([32])\n",
    "        h_conv1 = tf.nn.relu(conv2d(image, W_conv1) + b_conv1)\n",
    "        h_pool1 = max_pool_2x2(h_conv1)\n",
    "        endpoints['conv1'] = h_conv1\n",
    "        endpoints['conv1_pool1'] = h_pool1\n",
    "    \n",
    "    with tf.variable_scope('conv2') as scope:\n",
    "        W_conv2 = weight_variable([5, 5, 32, 64])\n",
    "        b_conv2 = bias_variable([64])\n",
    "        h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "        h_pool2 = max_pool_2x2(h_conv2)\n",
    "        endpoints['conv2'] = h_conv2\n",
    "        endpoints['conv2_pool2'] = h_pool2\n",
    "\n",
    "    with tf.variable_scope('fc1') as scope:\n",
    "        W_fc1 = weight_variable([8*8*64, 1024])\n",
    "        b_fc1 = bias_variable([1024])\n",
    "        h_pool2_flat = tf.reshape(h_pool2, [-1, 8*8*64])\n",
    "        h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n",
    "        h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "        endpoints['fc1'] = h_fc1_drop\n",
    "    \n",
    "    with tf.variable_scope('fc2') as scope:\n",
    "        W_fc2 = weight_variable([1024, 43])\n",
    "        b_fc2 = bias_variable([43])\n",
    "        logits = tf.matmul(h_fc1_drop, W_fc2) + b_fc2\n",
    "        endpoints['logits'] = logits\n",
    "    \n",
    "    return logits, endpoints\n",
    "\n",
    "def model_fourlayers():\n",
    "    endpoints = {}\n",
    "    with tf.variable_scope('conv1') as scope:\n",
    "        W_conv1 = weight_variable([5, 5, 1, 32])\n",
    "        b_conv1 = bias_variable([32])\n",
    "        h_conv1 = tf.nn.relu(conv2d(image, W_conv1) + b_conv1)\n",
    "        h_pool1 = max_pool_2x2(h_conv1)\n",
    "        endpoints['conv1'] = h_conv1\n",
    "        endpoints['conv1_pool1'] = h_pool1\n",
    "    \n",
    "    with tf.variable_scope('conv2') as scope:\n",
    "        W_conv2 = weight_variable([5, 5, 32, 64])\n",
    "        b_conv2 = bias_variable([64])\n",
    "        h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "        h_pool2 = max_pool_2x2(h_conv2)\n",
    "        endpoints['conv2'] = h_conv2\n",
    "        endpoints['conv2_pool2'] = h_pool2\n",
    "    \n",
    "    with tf.variable_scope('conv3') as scope:\n",
    "        W_conv3 = weight_variable([5, 5, 64, 128])\n",
    "        b_conv3 = bias_variable([128])\n",
    "        h_conv3 = tf.nn.relu(conv2d(h_pool2, W_conv3) + b_conv3)\n",
    "        h_pool3 = max_pool_2x2(h_conv3)\n",
    "        endpoints['conv3'] = h_conv3\n",
    "        endpoints['conv3_pool3'] = h_pool3\n",
    "    \n",
    "    with tf.variable_scope('conv4') as scope:\n",
    "        W_conv4 = weight_variable([3, 3, 128, 256])\n",
    "        b_conv4 = bias_variable([256])\n",
    "        h_conv4 = tf.nn.relu(conv2d(h_pool3, W_conv4) + b_conv4)\n",
    "        h_pool4 = max_pool_2x2(h_conv4)\n",
    "        endpoints['conv4'] = h_conv4\n",
    "        endpoints['conv4_pool4'] = h_pool4\n",
    "\n",
    "    with tf.variable_scope('fc1') as scope:\n",
    "        W_fc1 = weight_variable([2*2*256, 512])\n",
    "        b_fc1 = bias_variable([512])\n",
    "        h_pool4_flat = tf.reshape(h_pool4, [-1, 2*2*256])\n",
    "        h_fc1 = tf.nn.relu(tf.matmul(h_pool4_flat, W_fc1) + b_fc1)\n",
    "        h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "        endpoints['fc1'] = h_fc1_drop\n",
    "    \n",
    "    with tf.variable_scope('fc2') as scope:\n",
    "        W_fc2 = weight_variable([512, 43])\n",
    "        b_fc2 = bias_variable([43])\n",
    "        logits = tf.matmul(h_fc1_drop, W_fc2) + b_fc2\n",
    "        endpoints['logits'] = logits\n",
    "    \n",
    "    return logits, endpoints\n",
    "\n",
    "def loss(logits, labels):\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, labels))\n",
    "    return loss\n",
    "\n",
    "def training(loss, learning_rate, name = 'Adam'):\n",
    "    if name == 'GD':\n",
    "        optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "    else:\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
    "    \n",
    "    return optimizer\n",
    "\n",
    "def evaluation(logits, labels):\n",
    "    correct_prediction = tf.equal(tf.argmax(logits,1), tf.argmax(labels,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('Model Architecture for two layers is as follows')\n",
    "_, conv_dict1 = model_twolayers()\n",
    "conv_dict1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('Model Architecture for four layers is as follows')\n",
    "_, conv_dict2 = model_fourlayers()\n",
    "conv_dict2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3\n",
    "\n",
    "_What does your final architecture look like? (Type of model, layers, sizes, connectivity, etc.)  For reference on how to build a deep neural network using TensorFlow, see [Deep Neural Network in TensorFlow\n",
    "](https://classroom.udacity.com/nanodegrees/nd013/parts/fbf77062-5703-404e-b60c-95b78b2f3f9e/modules/6df7ae49-c61c-4bb2-a23e-6527e69209ec/lessons/b516a270-8600-4f93-a0a3-20dfeabe5da6/concepts/83a3a2a2-a9bd-4b7b-95b0-eb924ab14432) from the classroom._\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import time\n",
    "\n",
    "def run_training(steps,batch_size,learning_rate,k_prob,use_jitter=False, modelname ='Two'):\n",
    "    stime = time.time()\n",
    "    sess = tf.InteractiveSession()\n",
    "\n",
    "    if modelname == 'Four':\n",
    "        logits, _ = model_fourlayers()\n",
    "    else:\n",
    "        logits, _ = model_twolayers()\n",
    "    \n",
    "    losses = loss(logits,labels)\n",
    "    train_op = training(losses, learning_rate)\n",
    "    acc = evaluation(logits,labels)\n",
    "    tf.initialize_all_variables().run()\n",
    "    \n",
    "    train_acc_history = []\n",
    "    val_acc_history = []\n",
    "    loss_history = []\n",
    "    log_batch_step = 1000\n",
    "    batches_history = []\n",
    "       \n",
    "    if not use_jitter:\n",
    "        norm_train_features = preprocess_images(train_features)\n",
    "    else:\n",
    "        norm_train_features = train_features\n",
    "    \n",
    "    norm_valid_features = preprocess_images(valid_features)\n",
    "    norn_test_features = preprocess_images(X_test[1:1000])\n",
    "    \n",
    "    valid_feed_dict = {features: norm_valid_features, labels : valid_labels, keep_prob: k_prob}\n",
    "    test_feed_dict = {features: norn_test_features, labels : test_labels[1:1000], keep_prob: 1}\n",
    "    \n",
    "    print('Training model with {} layers started for step: {} batchsize: {} learning_rate: {} keep_prob: {}'.format(modelname, steps,batch_size,learning_rate,k_prob))\n",
    "    \n",
    "    for step in range(steps):\n",
    "        # Get a batch of training features and labels\n",
    "        #batch_start = batch_i*batch_size\n",
    "        #batch_features = train_features[batch_start:batch_start + batch_size]\n",
    "        #batch_labels = train_labels[batch_start:batch_start + batch_size]\n",
    "        batch_start = np.random.choice(norm_train_features.shape[0],batch_size)\n",
    "        batch_features = norm_train_features[batch_start]\n",
    "        batch_labels = train_labels[batch_start]\n",
    "        if use_jitter:\n",
    "            batch_features, batch_labels = jitter_image_data(batch_features, batch_labels, batch_size)\n",
    "\n",
    "        # Run optimizer\n",
    "        loss_value = sess.run(train_op, feed_dict={features: batch_features, labels: batch_labels, keep_prob: k_prob})\n",
    "                \n",
    "        if step%log_batch_step == 0:\n",
    "            train_accuracy = acc.eval(feed_dict={features:batch_features, labels: batch_labels, keep_prob: 1})\n",
    "            valid_accuracy = acc.eval(feed_dict=valid_feed_dict)\n",
    "            test_accuracy = acc.eval(feed_dict=test_feed_dict)\n",
    "            print(\"Steps %d, training accuracy: %g  validation accuracy: %g test accuracy: %g\"%(step, train_accuracy, valid_accuracy, test_accuracy))\n",
    "            previous_batch = batches_history[-1] if batches_history else 0\n",
    "            batches_history.append(log_batch_step + previous_batch)\n",
    "            train_acc_history.append(train_accuracy)\n",
    "            val_acc_history.append(valid_accuracy)\n",
    "            loss_history.append(loss_value)\n",
    "        \n",
    "    # Check accuracy against Test data\n",
    "    test_accuracy = sess.run(acc, feed_dict=test_feed_dict)\n",
    "    \n",
    "    print('Training completed with test accuracy : {}'.format(test_accuracy))\n",
    "    \n",
    "    return batches_history, train_acc_history, val_acc_history, test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# fine tuning hyperparameters\n",
    "jit = [False]\n",
    "models = ['Two']\n",
    "#models = ['Four']\n",
    "steps = [10000]\n",
    "#batches = [64, 128, 256]\n",
    "batches = [128]\n",
    "#lr = [1e-2, 1e-3, 1e-4]\n",
    "lr = [1e-3]\n",
    "#kp = [0.5, 0.6, 0,7, 0.8]\n",
    "kp = [0.5]\n",
    "result = []\n",
    "best_acc = 0.0\n",
    "best_params = []\n",
    "for j in jit:\n",
    "    for m in models:\n",
    "        for s in steps:\n",
    "            for b in batches:\n",
    "                for l in lr:\n",
    "                    for k in kp:\n",
    "                        batch_val, train_acc, val_acc, test_acc = run_training(s,b,l,k,j,m)\n",
    "                        if test_acc > best_acc:\n",
    "                            best_acc = test_acc\n",
    "                            best_params = [s,b,l,k,m]\n",
    "                            batch_best = batch_val\n",
    "                            acc_best = [train_acc, val_acc]\n",
    "\n",
    "print('Best Validation Accuracy : {}'.format(best_acc))\n",
    "print('Best parameters: steps:{} batches:{} learning_rate:{} keep_prob:{}'.format(best_params[0],best_params[1],best_params[2],best_params[3]))\n",
    "\n",
    "acc_plot = plt.subplot()\n",
    "acc_plot.set_title('Accuracy')\n",
    "acc_plot.plot(batch_best, acc_best[0], 'g', label='Training Accuracy')\n",
    "acc_plot.plot(batch_best, acc_best[1], 'r', label='Validation Accuracy')\n",
    "acc_plot.legend(loc=4)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# fine tuning hyperparameters\n",
    "# fine tuning hyperparameters\n",
    "jit = [True]\n",
    "models = ['Four']\n",
    "steps = [10000]\n",
    "batches = [64, 128]\n",
    "lr = [1e-2, 1e-3, 1e-4]\n",
    "kp = [0.5, 0.65, 0.8]\n",
    "result = []\n",
    "best_acc = 0.0\n",
    "best_params = []\n",
    "for j in jit:\n",
    "    for m in models:\n",
    "        for s in steps:\n",
    "            for b in batches:\n",
    "                for l in lr:\n",
    "                    for k in kp:\n",
    "                        batch_val, train_acc, val_acc, test_acc = run_training(s,b,l,k,j,m)\n",
    "                        if test_acc > best_acc:\n",
    "                            best_acc = test_acc\n",
    "                            best_params = [s,b,l,k,m]\n",
    "                            batch_best = batch_val\n",
    "                            acc_best = [train_acc, val_acc]\n",
    "\n",
    "print('Best Validation Accuracy : {}'.format(best_acc))\n",
    "print('Best parameters: steps:{} batches:{} learning_rate:{} keep_prob:{}'.format(best_params[0],best_params[1],best_params[2],best_params[3]))\n",
    "\n",
    "acc_plot = plt.subplot()\n",
    "acc_plot.set_title('Accuracy')\n",
    "acc_plot.plot(batch_best, acc_best[0], 'g', label='Training Accuracy')\n",
    "acc_plot.plot(batch_best, acc_best[1], 'r', label='Validation Accuracy')\n",
    "acc_plot.legend(loc=4)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Placeholder definition\n",
    "import tensorflow as tf\n",
    "currGraph = tf.Graph()\n",
    "with currGraph.as_default():\n",
    "    features = tf.placeholder(tf.float32, [None, 32, 32])\n",
    "    image = tf.reshape(features, [-1,32,32,1])\n",
    "    keep_prob = tf.placeholder(tf.float32)\n",
    "    labels = tf.placeholder(tf.float32, [None, 43])\n",
    "\n",
    "    def weight_variable(shape):\n",
    "      initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "      return tf.Variable(initial)\n",
    "\n",
    "    def bias_variable(shape):\n",
    "      initial = tf.constant(0.1, shape=shape)\n",
    "      return tf.Variable(initial)\n",
    "\n",
    "    def conv2d(x, W):\n",
    "      return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "    def max_pool_2x2(x):\n",
    "      return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n",
    "                            strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "    def model_twolayers():\n",
    "        endpoints = {}\n",
    "        with tf.variable_scope('conv1') as scope:\n",
    "            W_conv1 = weight_variable([5, 5, 1, 32])\n",
    "            b_conv1 = bias_variable([32])\n",
    "            h_conv1 = tf.nn.relu(conv2d(image, W_conv1) + b_conv1)\n",
    "            h_pool1 = max_pool_2x2(h_conv1)\n",
    "            endpoints['conv1'] = h_conv1\n",
    "            endpoints['conv1_pool1'] = h_pool1\n",
    "\n",
    "        with tf.variable_scope('conv2') as scope:\n",
    "            W_conv2 = weight_variable([5, 5, 32, 64])\n",
    "            b_conv2 = bias_variable([64])\n",
    "            h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "            h_pool2 = max_pool_2x2(h_conv2)\n",
    "            endpoints['conv2'] = h_conv2\n",
    "            endpoints['conv2_pool2'] = h_pool2\n",
    "\n",
    "        with tf.variable_scope('fc1') as scope:\n",
    "            W_fc1 = weight_variable([8*8*64, 1024])\n",
    "            b_fc1 = bias_variable([1024])\n",
    "            h_pool2_flat = tf.reshape(h_pool2, [-1, 8*8*64])\n",
    "            h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n",
    "            h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "            endpoints['fc1'] = h_fc1_drop\n",
    "\n",
    "        with tf.variable_scope('fc2') as scope:\n",
    "            W_fc2 = weight_variable([1024, 43])\n",
    "            b_fc2 = bias_variable([43])\n",
    "            logits = tf.matmul(h_fc1_drop, W_fc2) + b_fc2\n",
    "            endpoints['logits'] = logits\n",
    "\n",
    "        return logits, endpoints\n",
    "\n",
    "    def model_fourlayers():\n",
    "        endpoints = {}\n",
    "        with tf.variable_scope('conv1') as scope:\n",
    "            W_conv1 = weight_variable([5, 5, 1, 32])\n",
    "            b_conv1 = bias_variable([32])\n",
    "            h_conv1 = tf.nn.relu(conv2d(image, W_conv1) + b_conv1)\n",
    "            h_pool1 = max_pool_2x2(h_conv1)\n",
    "            endpoints['conv1'] = h_conv1\n",
    "            endpoints['conv1_pool1'] = h_pool1\n",
    "\n",
    "        with tf.variable_scope('conv2') as scope:\n",
    "            W_conv2 = weight_variable([5, 5, 32, 64])\n",
    "            b_conv2 = bias_variable([64])\n",
    "            h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "            h_pool2 = max_pool_2x2(h_conv2)\n",
    "            endpoints['conv2'] = h_conv2\n",
    "            endpoints['conv2_pool2'] = h_pool2\n",
    "\n",
    "        with tf.variable_scope('conv3') as scope:\n",
    "            W_conv3 = weight_variable([5, 5, 64, 128])\n",
    "            b_conv3 = bias_variable([128])\n",
    "            h_conv3 = tf.nn.relu(conv2d(h_pool2, W_conv3) + b_conv3)\n",
    "            h_pool3 = max_pool_2x2(h_conv3)\n",
    "            endpoints['conv3'] = h_conv3\n",
    "            endpoints['conv3_pool3'] = h_pool3\n",
    "\n",
    "        with tf.variable_scope('conv4') as scope:\n",
    "            W_conv4 = weight_variable([3, 3, 128, 256])\n",
    "            b_conv4 = bias_variable([256])\n",
    "            h_conv4 = tf.nn.relu(conv2d(h_pool3, W_conv4) + b_conv4)\n",
    "            h_pool4 = max_pool_2x2(h_conv4)\n",
    "            endpoints['conv4'] = h_conv4\n",
    "            endpoints['conv4_pool4'] = h_pool4\n",
    "\n",
    "        with tf.variable_scope('fc1') as scope:\n",
    "            W_fc1 = weight_variable([2*2*256, 512])\n",
    "            b_fc1 = bias_variable([512])\n",
    "            h_pool4_flat = tf.reshape(h_pool4, [-1, 2*2*256])\n",
    "            h_fc1 = tf.nn.relu(tf.matmul(h_pool4_flat, W_fc1) + b_fc1)\n",
    "            h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "            endpoints['fc1'] = h_fc1_drop\n",
    "\n",
    "        with tf.variable_scope('fc2') as scope:\n",
    "            W_fc2 = weight_variable([512, 43])\n",
    "            b_fc2 = bias_variable([43])\n",
    "            logits = tf.matmul(h_fc1_drop, W_fc2) + b_fc2\n",
    "            endpoints['logits'] = logits\n",
    "\n",
    "        return logits, endpoints\n",
    "\n",
    "    def loss(logits, labels):\n",
    "        loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, labels))\n",
    "        return loss\n",
    "\n",
    "    def training(loss, learning_rate, name = 'Adam'):\n",
    "        if name == 'GD':\n",
    "            optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "        else:\n",
    "            optimizer = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "        return optimizer\n",
    "\n",
    "    def evaluation(logits, labels):\n",
    "        correct_prediction = tf.equal(tf.argmax(logits,1), tf.argmax(labels,1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "        return accuracy\n",
    "\n",
    "print('Graph session created to train a longer model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "curr_path = os.getcwd()\n",
    "save_path = curr_path +'/models/model.ckpt-99999'\n",
    "use_jitter = False\n",
    "steps = 100000\n",
    "batch_size = 128\n",
    "k_prob = 0.5\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Data preparation \n",
    "if not use_jitter:\n",
    "    norm_train_features = preprocess_images(train_features)\n",
    "else:\n",
    "    norm_train_features = train_features\n",
    "    \n",
    "norm_valid_features = preprocess_images(valid_features)\n",
    "norm_test_features = preprocess_images(X_test)\n",
    "n_test = norm_test_features.shape[0]\n",
    "    \n",
    "valid_feed_dict = {features: norm_valid_features, labels : valid_labels, keep_prob: k_prob}\n",
    "#test_feed_dict = {features: norm_test_features[1:1000], labels : test_labels[1:1000], keep_prob: 1}\n",
    "\n",
    "restore_model = 1\n",
    "\n",
    "\n",
    "with currGraph.as_default():  \n",
    "    logits, _ = model_twolayers()\n",
    "    losses = loss(logits,labels)\n",
    "    train_op = training(losses, learning_rate)\n",
    "    acc = evaluation(logits,labels)\n",
    "    init = tf.initialize_all_variables()\n",
    "    # Create a saver for writing training checkpoints.\n",
    "    saver = tf.train.Saver()\n",
    "    # Create a session for running Ops on the Graph.\n",
    "    sess = tf.Session()\n",
    "    # Run the Op to initialize the variables.\n",
    "        #use the previous model or don't and initialize variables\n",
    "    if restore_model:\n",
    "        saver.restore(sess,save_path)\n",
    "        print(\"Model restored.\")\n",
    "    else:\n",
    "        sess.run(init)\n",
    "    \n",
    "    if 0:\n",
    "    \n",
    "        for step in range(steps):\n",
    "            batch_start = np.random.choice(norm_train_features.shape[0],batch_size)\n",
    "            batch_features = norm_train_features[batch_start]\n",
    "            batch_labels = train_labels[batch_start]\n",
    "\n",
    "            if use_jitter:\n",
    "                batch_features, batch_labels = jitter_image_data(batch_features, batch_labels, batch_size)\n",
    "            # Run optimizer\n",
    "            loss_value = sess.run(train_op, feed_dict={features: batch_features, labels: batch_labels, keep_prob: k_prob})\n",
    "\n",
    "            if step%100 == 0:\n",
    "                train_accuracy = sess.run([acc], feed_dict={features:batch_features, labels: batch_labels, keep_prob: 1})\n",
    "                valid_accuracy = sess.run([acc], feed_dict=valid_feed_dict)\n",
    "                batch_count = int(math.ceil(n_test/batch_size))\n",
    "                total = 0\n",
    "                for i in range(batch_count):\n",
    "                    batch_start = i*batch_size\n",
    "                    test_batch_features = norm_test_features[batch_start:batch_start + batch_size]\n",
    "                    test_batch_labels = test_labels[batch_start:batch_start + batch_size]\n",
    "                    total += sess.run(acc, feed_dict={features:test_batch_features, labels: test_batch_labels, keep_prob: 1})\n",
    "                    #print(total)\n",
    "                #print(total,n_test)\n",
    "                test_accuracy = total / batch_count\n",
    "                #print(\"Steps %d, training accuracy: %g  validation accuracy: %g test accuracy: %g\"%(step, train_accuracy, valid_accuracy, test_accuracy))\n",
    "                print(\"Steps {}, training accuracy: {}  validation accuracy: {} test accuracy: {}\".format(step, train_accuracy, valid_accuracy, test_accuracy))\n",
    "            if ((step == (steps-1)) or (test_accuracy > 0.99)):\n",
    "                batch_count = int(math.ceil(n_test/batch_size))\n",
    "                total = 0\n",
    "                for i in range(batch_count):\n",
    "                    batch_start = i*batch_size\n",
    "                    test_batch_features = norm_test_features[batch_start:batch_start + batch_size]\n",
    "                    test_batch_labels = test_labels[batch_start:batch_start + batch_size]\n",
    "                    total += sess.run(acc, feed_dict={features:test_batch_features, labels: test_batch_labels, keep_prob: 1})\n",
    "                test_accuracy = total / batch_count\n",
    "                print('Final test accuracy: {}'.format(test_accuracy))\n",
    "                save_path = saver.save(sess,save_path, global_step=step)\n",
    "                print(\"Model saved.\")\n",
    "    else:\n",
    "        print('Success')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "curr_path = os.getcwd()\n",
    "save_path = curr_path +'/model_jitter.ckpt'\n",
    "use_jitter = True\n",
    "steps = 100000\n",
    "batch_size = 64\n",
    "k_prob = 0.65\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Data preparation \n",
    "if not use_jitter:\n",
    "    norm_train_features = preprocess_images(train_features)\n",
    "else:\n",
    "    norm_train_features = train_features\n",
    "    \n",
    "norm_valid_features = preprocess_images(valid_features)\n",
    "norm_test_features = preprocess_images(X_test)\n",
    "n_test = norm_test_features.shape[0]\n",
    "    \n",
    "valid_feed_dict = {features: norm_valid_features, labels : valid_labels, keep_prob: k_prob}\n",
    "#test_feed_dict = {features: norm_test_features[1:1000], labels : test_labels[1:1000], keep_prob: 1}\n",
    "\n",
    "restore_model = 0\n",
    "\n",
    "\n",
    "with currGraph.as_default():\n",
    "     \n",
    "    #use the previous model or don't and initialize variables\n",
    "    if restore_model:\n",
    "        saver.restore(sess,save_path)\n",
    "        print(\"Model restored.\")\n",
    "    \n",
    "    logits, _ = model_twolayers()\n",
    "    losses = loss(logits,labels)\n",
    "    train_op = training(losses, learning_rate)\n",
    "    acc = evaluation(logits,labels)\n",
    "    init = tf.initialize_all_variables()\n",
    "    # Create a saver for writing training checkpoints.\n",
    "    saver = tf.train.Saver()\n",
    "    # Create a session for running Ops on the Graph.\n",
    "    sess = tf.Session()\n",
    "    # Run the Op to initialize the variables.\n",
    "    sess.run(init)    \n",
    "    \n",
    "    for step in range(steps):\n",
    "        batch_start = np.random.choice(norm_train_features.shape[0],batch_size)\n",
    "        batch_features = norm_train_features[batch_start]\n",
    "        batch_labels = train_labels[batch_start]\n",
    "        \n",
    "        if use_jitter:\n",
    "            batch_features, batch_labels = jitter_image_data(batch_features, batch_labels, batch_size)\n",
    "        # Run optimizer\n",
    "        loss_value = sess.run(train_op, feed_dict={features: batch_features, labels: batch_labels, keep_prob: k_prob})\n",
    "\n",
    "        if step%100 == 0:\n",
    "            train_accuracy = sess.run([acc], feed_dict={features:batch_features, labels: batch_labels, keep_prob: 1})\n",
    "            valid_accuracy = sess.run([acc], feed_dict=valid_feed_dict)\n",
    "            batch_count = int(math.ceil(n_test/batch_size))\n",
    "            total = 0\n",
    "            for i in range(batch_count):\n",
    "                batch_start = i*batch_size\n",
    "                test_batch_features = norm_test_features[batch_start:batch_start + batch_size]\n",
    "                test_batch_labels = test_labels[batch_start:batch_start + batch_size]\n",
    "                total += sess.run(acc, feed_dict={features:test_batch_features, labels: test_batch_labels, keep_prob: 1})\n",
    "                #print(total)\n",
    "            #print(total,n_test)\n",
    "            test_accuracy = total / batch_count\n",
    "            #print(\"Steps %d, training accuracy: %g  validation accuracy: %g test accuracy: %g\"%(step, train_accuracy, valid_accuracy, test_accuracy))\n",
    "            print(\"Steps {}, training accuracy: {}  validation accuracy: {} test accuracy: {}\".format(step, train_accuracy, valid_accuracy, test_accuracy))\n",
    "            \n",
    "        if ((step == (steps-1)) or (test_accuracy > 0.99)):\n",
    "            batch_count = int(math.ceil(n_test/batch_size))\n",
    "            total = 0\n",
    "            for i in range(batch_count):\n",
    "                batch_start = i*batch_size\n",
    "                test_batch_features = norm_test_features[batch_start:batch_start + batch_size]\n",
    "                test_batch_labels = test_labels[batch_start:batch_start + batch_size]\n",
    "                total += sess.run(acc, feed_dict={features:test_batch_features, labels: test_batch_labels, keep_prob: 1})\n",
    "            test_accuracy = total / batch_count\n",
    "            print('Final test accuracy: {}'.format(test_accuracy))\n",
    "            save_path = saver.save(sess,save_path, global_step=step)\n",
    "            print(\"Model saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "curr_path = os.getcwd()\n",
    "save_path = curr_path +'/modelFour_jitter.ckpt'\n",
    "use_jitter = True\n",
    "steps = 100000\n",
    "batch_size = 128\n",
    "k_prob = 0.65\n",
    "learning_rate = 0.0001\n",
    "\n",
    "# Data preparation \n",
    "if not use_jitter:\n",
    "    norm_train_features = preprocess_images(train_features)\n",
    "else:\n",
    "    norm_train_features = train_features\n",
    "    \n",
    "norm_valid_features = preprocess_images(valid_features)\n",
    "norm_test_features = preprocess_images(X_test)\n",
    "n_test = norm_test_features.shape[0]\n",
    "    \n",
    "valid_feed_dict = {features: norm_valid_features, labels : valid_labels, keep_prob: k_prob}\n",
    "#test_feed_dict = {features: norm_test_features[1:1000], labels : test_labels[1:1000], keep_prob: 1}\n",
    "\n",
    "restore_model = 0\n",
    "\n",
    "\n",
    "with currGraph.as_default():\n",
    "     \n",
    "    #use the previous model or don't and initialize variables\n",
    "    if restore_model:\n",
    "        saver.restore(sess,save_path)\n",
    "        print(\"Model restored.\")\n",
    "    \n",
    "    logits, _ = model_fourlayers()\n",
    "    losses = loss(logits,labels)\n",
    "    train_op = training(losses, learning_rate)\n",
    "    acc = evaluation(logits,labels)\n",
    "    init = tf.initialize_all_variables()\n",
    "    # Create a saver for writing training checkpoints.\n",
    "    saver = tf.train.Saver()\n",
    "    # Create a session for running Ops on the Graph.\n",
    "    sess = tf.Session()\n",
    "    # Run the Op to initialize the variables.\n",
    "    sess.run(init)    \n",
    "    \n",
    "    for step in range(steps):\n",
    "        batch_start = np.random.choice(norm_train_features.shape[0],batch_size)\n",
    "        batch_features = norm_train_features[batch_start]\n",
    "        batch_labels = train_labels[batch_start]\n",
    "        \n",
    "        if use_jitter:\n",
    "            batch_features, batch_labels = jitter_image_data(batch_features, batch_labels, batch_size)\n",
    "        # Run optimizer\n",
    "        loss_value = sess.run(train_op, feed_dict={features: batch_features, labels: batch_labels, keep_prob: k_prob})\n",
    "\n",
    "        if step%100 == 0:\n",
    "            train_accuracy = sess.run([acc], feed_dict={features:batch_features, labels: batch_labels, keep_prob: 1})\n",
    "            valid_accuracy = sess.run([acc], feed_dict=valid_feed_dict)\n",
    "            batch_count = int(math.ceil(n_test/batch_size))\n",
    "            total = 0\n",
    "            for i in range(batch_count):\n",
    "                batch_start = i*batch_size\n",
    "                test_batch_features = norm_test_features[batch_start:batch_start + batch_size]\n",
    "                test_batch_labels = test_labels[batch_start:batch_start + batch_size]\n",
    "                total += sess.run(acc, feed_dict={features:test_batch_features, labels: test_batch_labels, keep_prob: 1})\n",
    "                #print(total)\n",
    "            #print(total,n_test)\n",
    "            test_accuracy = total / batch_count\n",
    "            #print(\"Steps %d, training accuracy: %g  validation accuracy: %g test accuracy: %g\"%(step, train_accuracy, valid_accuracy, test_accuracy))\n",
    "            print(\"Steps {}, training accuracy: {}  validation accuracy: {} test accuracy: {}\".format(step, train_accuracy, valid_accuracy, test_accuracy))\n",
    "            \n",
    "        if ((step == (steps-1)) or (test_accuracy > 0.99)):\n",
    "            batch_count = int(math.ceil(n_test/batch_size))\n",
    "            total = 0\n",
    "            for i in range(batch_count):\n",
    "                batch_start = i*batch_size\n",
    "                test_batch_features = norm_test_features[batch_start:batch_start + batch_size]\n",
    "                test_batch_labels = test_labels[batch_start:batch_start + batch_size]\n",
    "                total += sess.run(acc, feed_dict={features:test_batch_features, labels: test_batch_labels, keep_prob: 1})\n",
    "            test_accuracy = total / batch_count\n",
    "            print('Final test accuracy: {}'.format(test_accuracy))\n",
    "            save_path = saver.save(sess,save_path, global_step=step)\n",
    "            print(\"Model saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Placeholder definition\n",
    "import tensorflow as tf\n",
    "currGraph_color = tf.Graph()\n",
    "with currGraph_color.as_default():\n",
    "    features = tf.placeholder(tf.float32, [None, 32, 32, 3])\n",
    "    image = tf.reshape(features, [-1,32,32,3])\n",
    "    keep_prob = tf.placeholder(tf.float32)\n",
    "    labels = tf.placeholder(tf.float32, [None, 43])\n",
    "\n",
    "    def weight_variable(shape):\n",
    "      initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "      return tf.Variable(initial)\n",
    "\n",
    "    def bias_variable(shape):\n",
    "      initial = tf.constant(0.1, shape=shape)\n",
    "      return tf.Variable(initial)\n",
    "\n",
    "    def conv2d(x, W):\n",
    "      return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "    def max_pool_2x2(x):\n",
    "      return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n",
    "                            strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "    def model_twolayers():\n",
    "        endpoints = {}\n",
    "        with tf.variable_scope('conv1') as scope:\n",
    "            W_conv1 = weight_variable([5, 5, 3, 32])\n",
    "            b_conv1 = bias_variable([32])\n",
    "            h_conv1 = tf.nn.relu(conv2d(image, W_conv1) + b_conv1)\n",
    "            h_pool1 = max_pool_2x2(h_conv1)\n",
    "            endpoints['conv1'] = h_conv1\n",
    "            endpoints['conv1_pool1'] = h_pool1\n",
    "\n",
    "        with tf.variable_scope('conv2') as scope:\n",
    "            W_conv2 = weight_variable([5, 5, 32, 64])\n",
    "            b_conv2 = bias_variable([64])\n",
    "            h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "            h_pool2 = max_pool_2x2(h_conv2)\n",
    "            endpoints['conv2'] = h_conv2\n",
    "            endpoints['conv2_pool2'] = h_pool2\n",
    "\n",
    "        with tf.variable_scope('fc1') as scope:\n",
    "            W_fc1 = weight_variable([8*8*64, 1024])\n",
    "            b_fc1 = bias_variable([1024])\n",
    "            h_pool2_flat = tf.reshape(h_pool2, [-1, 8*8*64])\n",
    "            h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n",
    "            h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "            endpoints['fc1'] = h_fc1_drop\n",
    "\n",
    "        with tf.variable_scope('fc2') as scope:\n",
    "            W_fc2 = weight_variable([1024, 43])\n",
    "            b_fc2 = bias_variable([43])\n",
    "            logits = tf.matmul(h_fc1_drop, W_fc2) + b_fc2\n",
    "            endpoints['logits'] = logits\n",
    "\n",
    "        return logits, endpoints\n",
    "\n",
    "    def loss(logits, labels):\n",
    "        loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, labels))\n",
    "        return loss\n",
    "\n",
    "    def training(loss, learning_rate, name = 'Adam'):\n",
    "        if name == 'GD':\n",
    "            optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "        else:\n",
    "            optimizer = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "        return optimizer\n",
    "\n",
    "    def evaluation(logits, labels):\n",
    "        correct_prediction = tf.equal(tf.argmax(logits,1), tf.argmax(labels,1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "        return accuracy\n",
    "\n",
    "print('Graph session created to train a longer model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "curr_path = os.getcwd()\n",
    "save_path = curr_path +'/color_model_jitter.ckpt'\n",
    "use_jitter = True\n",
    "steps = 100000\n",
    "batch_size = 128\n",
    "k_prob = 0.5\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Data preparation \n",
    "if not use_jitter:\n",
    "    norm_train_features = preprocess_images(train_features)\n",
    "else:\n",
    "    norm_train_features = train_features\n",
    "    \n",
    "norm_valid_features = preprocess_images(valid_features)\n",
    "norm_test_features = preprocess_images(X_test)\n",
    "n_test = norm_test_features.shape[0]\n",
    "    \n",
    "valid_feed_dict = {features: norm_valid_features, labels : valid_labels, keep_prob: k_prob}\n",
    "#test_feed_dict = {features: norm_test_features[1:1000], labels : test_labels[1:1000], keep_prob: 1}\n",
    "\n",
    "restore_model = 0\n",
    "\n",
    "\n",
    "with currGraph_color.as_default():\n",
    "     \n",
    "    #use the previous model or don't and initialize variables\n",
    "    if restore_model:\n",
    "        saver.restore(sess,save_path)\n",
    "        print(\"Model restored.\")\n",
    "    \n",
    "    logits, _ = model_twolayers()\n",
    "    losses = loss(logits,labels)\n",
    "    train_op = training(losses, learning_rate)\n",
    "    acc = evaluation(logits,labels)\n",
    "    init = tf.initialize_all_variables()\n",
    "    # Create a saver for writing training checkpoints.\n",
    "    saver = tf.train.Saver()\n",
    "    # Create a session for running Ops on the Graph.\n",
    "    sess = tf.Session()\n",
    "    # Run the Op to initialize the variables.\n",
    "    sess.run(init)    \n",
    "    \n",
    "    for step in range(steps):\n",
    "        batch_start = np.random.choice(norm_train_features.shape[0],batch_size)\n",
    "        batch_features = norm_train_features[batch_start]\n",
    "        batch_labels = train_labels[batch_start]\n",
    "        \n",
    "        if use_jitter:\n",
    "            batch_features, batch_labels = jitter_image_data(batch_features, batch_labels, batch_size)\n",
    "        # Run optimizer\n",
    "        loss_value = sess.run(train_op, feed_dict={features: batch_features, labels: batch_labels, keep_prob: k_prob})\n",
    "\n",
    "        if step%100 == 0:\n",
    "            train_accuracy = sess.run([acc], feed_dict={features:batch_features, labels: batch_labels, keep_prob: 1})\n",
    "            valid_accuracy = sess.run([acc], feed_dict=valid_feed_dict)\n",
    "            batch_count = int(math.ceil(n_test/batch_size))\n",
    "            total = 0\n",
    "            for i in range(batch_count):\n",
    "                batch_start = i*batch_size\n",
    "                test_batch_features = norm_test_features[batch_start:batch_start + batch_size]\n",
    "                test_batch_labels = test_labels[batch_start:batch_start + batch_size]\n",
    "                total += sess.run(acc, feed_dict={features:test_batch_features, labels: test_batch_labels, keep_prob: 1})\n",
    "                #print(total)\n",
    "            #print(total,n_test)\n",
    "            test_accuracy = total / batch_count\n",
    "            #print(\"Steps %d, training accuracy: %g  validation accuracy: %g test accuracy: %g\"%(step, train_accuracy, valid_accuracy, test_accuracy))\n",
    "            print(\"Steps {}, training accuracy: {}  validation accuracy: {} test accuracy: {}\".format(step, train_accuracy, valid_accuracy, test_accuracy))\n",
    "            \n",
    "        if ((step == (steps-1)) or (test_accuracy > 0.99)):\n",
    "            batch_count = int(math.ceil(n_test/batch_size))\n",
    "            total = 0\n",
    "            for i in range(batch_count):\n",
    "                batch_start = i*batch_size\n",
    "                test_batch_features = norm_test_features[batch_start:batch_start + batch_size]\n",
    "                test_batch_labels = test_labels[batch_start:batch_start + batch_size]\n",
    "                total += sess.run(acc, feed_dict={features:test_batch_features, labels: test_batch_labels, keep_prob: 1})\n",
    "            test_accuracy = total / batch_count\n",
    "            print('Final test accuracy: {}'.format(test_accuracy))\n",
    "            save_path = saver.save(sess,save_path, global_step=step)\n",
    "            print(\"Model saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4\n",
    "\n",
    "_How did you train your model? (Type of optimizer, batch size, epochs, hyperparameters, etc.)_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5\n",
    "\n",
    "\n",
    "_What approach did you take in coming up with a solution to this problem?_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 3: Test a Model on New Images\n",
    "\n",
    "Take several pictures of traffic signs that you find on the web or around you (at least five), and run them through your classifier on your computer to produce example results. The classifier might not recognize some local signs but it could prove interesting nonetheless.\n",
    "\n",
    "You may find `signnames.csv` useful as it contains mappings from the class id (integer) to the actual sign name."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation\n",
    "\n",
    "Use the code cell (or multiple code cells, if necessary) to implement the first step of your project. Once you have completed your implementation and are satisfied with the results, be sure to thoroughly answer the questions that follow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Load the images and plot them here.\n",
    "### Feel free to use as many code cells as needed.\n",
    "import glob\n",
    "import os\n",
    "import math\n",
    "curr_path = os.getcwd()\n",
    "save_path = curr_path +'/color_model_jitter.ckpt-99999'\n",
    "\n",
    "def read_new_images():\n",
    "    processed_images = []\n",
    "    for image_name in glob.glob('images/*.jpg'):\n",
    "        image = cv2.imread(image_name)\n",
    "        resize_image = cv2.resize(image,(32,32))\n",
    "        processed_images.append(resize_image)\n",
    "\n",
    "    return(preprocess_images(np.array(processed_images)))\n",
    "    \n",
    "new_test_images = read_new_images()\n",
    "\n",
    "curr_path = os.getcwd()\n",
    "save_path = curr_path +'/models/model.ckpt-99999'\n",
    "\n",
    "with tf.Session(graph=currGraph) as sess:\n",
    "    logits, _ = model_twolayers()\n",
    "    losses = loss(logits,labels)\n",
    "    train_op = training(losses, 0.001)\n",
    "    acc = evaluation(logits,labels)\n",
    "    init = tf.initialize_all_variables()\n",
    "    saver = tf.train.Saver()\n",
    "    saver.restore(sess,save_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6\n",
    "\n",
    "_Choose five candidate images of traffic signs and provide them in the report. Are there any particular qualities of the image(s) that might make classification difficult? It would be helpful to plot the images in the notebook._\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Run the predictions here.\n",
    "### Feel free to use as many code cells as needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 7\n",
    "\n",
    "_Is your model able to perform equally well on captured pictures or a live camera stream when compared to testing on the dataset?_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Visualize the softmax probabilities here.\n",
    "### Feel free to use as many code cells as needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 8\n",
    "\n",
    "*Use the model's softmax probabilities to visualize the **certainty** of its predictions, [`tf.nn.top_k`](https://www.tensorflow.org/versions/r0.11/api_docs/python/nn.html#top_k) could prove helpful here. Which predictions is the model certain of? Uncertain? If the model was incorrect in its initial prediction, does the correct prediction appear in the top k? (k should be 5 at most)*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 9\n",
    "_If necessary, provide documentation for how an interface was built for your model to load and classify newly-acquired images._\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Note**: Once you have completed all of the code implementations and successfully answered each question above, you may finalize your work by exporting the iPython Notebook as an HTML document. You can do this by using the menu above and navigating to  \\n\",\n",
    "    \"**File -> Download as -> HTML (.html)**. Include the finished document along with this notebook as your submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#http://stackoverflow.com/questions/10388462/matplotlib-different-size-subplots\n",
    "import glob\n",
    "from matplotlib import gridspec\n",
    "curr_path = os.getcwd()\n",
    "model_path = curr_path +'/models/model_simple.ckpt'\n",
    "def read_new_images():\n",
    "    processed_images = []\n",
    "    for image_name in glob.glob('images/*.jpg'):\n",
    "        image = cv2.imread(image_name)\n",
    "        resize_image = cv2.resize(image,(32,32))\n",
    "        processed_images.append(resize_image)\n",
    "\n",
    "    return(preprocess_images(np.array(processed_images)))\n",
    "    \n",
    "new_test_images = read_new_images()\n",
    "with tf.Session(graph=graph_model_simple) as sess:\n",
    "    saver.restore(sess,model_path)\n",
    "    pred_value, pred_class_value = sess.run([pred,pred_class],feed_dict={features:new_test_images,keep_prob:1})\n",
    "\n",
    "\n",
    "num_new_test_images = pred_value.shape[0]\n",
    "num_class_labels = pred_value.shape[1]\n",
    "# Due to memory constraint, displaying only 5 images\n",
    "rand_idx = np.random.choice(num_new_test_images,5)\n",
    "sample_prob = pred_value[rand_idx]\n",
    "sample_class = pred_class_value.indices[rand_idx]\n",
    "index = np.arange(num_class_labels)\n",
    "for idx in range(5):\n",
    "    fig = plt.figure(figsize=(10, 10)) \n",
    "    gs = gridspec.GridSpec(1, 2, height_ratios=[1, 5])\n",
    "    ax0 = plt.subplot(gs[0])\n",
    "    ax0.imshow(new_test_images[idx],cmap='gray')\n",
    "    ax0.set_xticks([])\n",
    "    ax0.set_yticks([])\n",
    "    ax1 = plt.subplot(gs[1])\n",
    "    ax1.bar(index, pred_value[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Placeholder definition\n",
    "import tensorflow as tf\n",
    "graph_model_multi = tf.Graph()\n",
    "\n",
    "with graph_model_multi.as_default():\n",
    "    \n",
    "    features = tf.placeholder(tf.float32, [None, 32, 32])\n",
    "    image = tf.reshape(features, [-1,32,32,1])\n",
    "    keep_prob = tf.placeholder(tf.float32)\n",
    "    lr_value = tf.placeholder(tf.float32)\n",
    "    labels = tf.placeholder(tf.float32, [None, 43])\n",
    "\n",
    "    def weight_variable(shape):\n",
    "      initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "      return tf.Variable(initial)\n",
    "\n",
    "    def bias_variable(shape):\n",
    "      initial = tf.constant(0.1, shape=shape)\n",
    "      return tf.Variable(initial)\n",
    "\n",
    "    def conv2d(x, W):\n",
    "      return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "    def max_pool_2x2(x):\n",
    "      return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n",
    "                            strides=[1, 2, 2, 1], padding='SAME')\n",
    "    # normalize\n",
    "    def norm(x):\n",
    "        return tf.nn.lrn(x, 4, bias=1.0, alpha=0.001/9.00, beta=0.75)\n",
    "\n",
    "    def model():\n",
    "        endpoints = {}\n",
    "        with tf.variable_scope('conv1') as scope:\n",
    "            W_conv1 = weight_variable([5, 5, 1, 8])\n",
    "            b_conv1 = bias_variable([8])\n",
    "            h_conv1 = tf.nn.relu(conv2d(image, W_conv1) + b_conv1)\n",
    "            h_pool1 = max_pool_2x2(h_conv1)\n",
    "            endpoints['conv1'] = h_conv1\n",
    "            endpoints['conv1_pool1'] = h_pool1\n",
    "\n",
    "        with tf.variable_scope('conv2') as scope:\n",
    "            W_conv2 = weight_variable([5, 5, 8, 16])\n",
    "            b_conv2 = bias_variable([16])\n",
    "            h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "            h_pool2 = max_pool_2x2(h_conv2)\n",
    "            endpoints['conv2'] = h_conv2\n",
    "            endpoints['conv2_pool2'] = h_pool2\n",
    "        \n",
    "        with tf.variable_scope('combined') as scope:\n",
    "            h_pool1_flat = tf.reshape(h_pool1, [-1, 16*16*8])\n",
    "            h_pool2_flat = tf.reshape(h_pool2, [-1, 8*8*16])\n",
    "            print(\"h_pool1_flat:\",h_pool1_flat.get_shape())\n",
    "            print(\"h_pool2_flat:\",h_pool2_flat.get_shape())\n",
    "            combined_pool_flat = tf.concat(1, [h_pool1_flat, h_pool2_flat])\n",
    "            combined_pool_flat_shape = combined_pool_flat.get_shape()[1].value\n",
    "            print(\"combined_pool_flat:\",combined_pool_flat.get_shape())\n",
    "            print(\"combined_pool_flat_shape:\",combined_pool_flat_shape)\n",
    "            endpoints['combined'] = combined_pool_flat\n",
    "    \n",
    "        with tf.variable_scope('fc1') as scope:\n",
    "            W_fc1 = weight_variable([combined_pool_flat_shape, 1024])\n",
    "            b_fc1 = bias_variable([1024])\n",
    "            h_fc1 = tf.nn.relu(tf.matmul(combined_pool_flat, W_fc1) + b_fc1)\n",
    "            h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "            endpoints['fc1'] = h_fc1_drop\n",
    "        \n",
    "        with tf.variable_scope('fc2') as scope:\n",
    "            W_fc2 = weight_variable([1024, 100])\n",
    "            b_fc2 = bias_variable([100])\n",
    "            h_fc2 = tf.nn.relu(tf.matmul(h_fc1_drop, W_fc2) + b_fc2)\n",
    "            h_fc2_drop = tf.nn.dropout(h_fc2, keep_prob)\n",
    "            endpoints['fc2'] = h_fc2_drop\n",
    "            \n",
    "        with tf.variable_scope('fc3') as scope:\n",
    "            W_fc3 = weight_variable([100, 43])\n",
    "            b_fc3 = bias_variable([43])\n",
    "            logits = tf.matmul(h_fc2_drop, W_fc3) + b_fc3\n",
    "            endpoints['logits'] = logits\n",
    "\n",
    "        return logits, endpoints        \n",
    "\n",
    "    def loss(logits, labels):\n",
    "        loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, labels))\n",
    "        return loss\n",
    "\n",
    "    def training(loss, learning_rate, name = 'Adam'):\n",
    "        if name == 'GD':\n",
    "            optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "        else:\n",
    "            optimizer = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "        return optimizer\n",
    "\n",
    "    def evaluation(logits, labels):\n",
    "        correct_prediction = tf.equal(tf.argmax(logits,1), tf.argmax(labels,1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "        return accuracy\n",
    "    \n",
    "    def prediction_prob(logits):\n",
    "        return tf.nn.softmax(logits)\n",
    "    \n",
    "    def prediction_class(logits):\n",
    "        return tf.nn.top_k(logits)\n",
    "    \n",
    "    logits, graphdef = model()\n",
    "    losses = loss(logits,labels)\n",
    "    train_op = training(losses, lr_value)\n",
    "    acc = evaluation(logits,labels)\n",
    "    pred = prediction_prob(logits)\n",
    "    pred_class = prediction_class(logits)\n",
    "    init = tf.initialize_all_variables()\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "print('MultiScale model defined..')\n",
    "graphdef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import time\n",
    "\n",
    "def run_training(steps,batch_size,learning_rate,k_prob,use_jitter=False, modelname ='Two'):\n",
    "    stime = time.time()\n",
    "    sess = tf.Session(graph=graph_model_multi2)\n",
    "    sess.run(init)\n",
    "    \n",
    "    train_acc_history = []\n",
    "    val_acc_history = []\n",
    "    loss_history = []\n",
    "    log_batch_step = 1000\n",
    "    batches_history = []\n",
    "       \n",
    "    if not use_jitter:\n",
    "        norm_train_features = preprocess_images(train_features)\n",
    "    else:\n",
    "        norm_train_features = train_features\n",
    "    \n",
    "    norm_valid_features = preprocess_images(valid_features)\n",
    "    norn_test_features = preprocess_images(X_test[1:1000])\n",
    "    \n",
    "    valid_feed_dict = {features: norm_valid_features, labels : valid_labels, keep_prob: k_prob, lr_value: learning_rate}\n",
    "    test_feed_dict = {features: norn_test_features, labels : test_labels[1:1000], keep_prob: 1, lr_value: learning_rate}\n",
    "    \n",
    "    print('Training model with {} layers started for step: {} batchsize: {} learning_rate: {} keep_prob: {}'.format(modelname, steps,batch_size,learning_rate,k_prob))\n",
    "    \n",
    "    for step in range(steps):\n",
    "        # Get a batch of training features and labels\n",
    "        #batch_start = batch_i*batch_size\n",
    "        #batch_features = train_features[batch_start:batch_start + batch_size]\n",
    "        #batch_labels = train_labels[batch_start:batch_start + batch_size]\n",
    "        batch_start = np.random.choice(norm_train_features.shape[0],batch_size)\n",
    "        batch_features = norm_train_features[batch_start]\n",
    "        batch_labels = train_labels[batch_start]\n",
    "        if use_jitter:\n",
    "            batch_features, batch_labels = jitter_image_data(batch_features, batch_labels, batch_size)\n",
    "\n",
    "        # Run optimizer\n",
    "        loss_value = sess.run(train_op, feed_dict={features: batch_features, labels: batch_labels, keep_prob: k_prob, lr_value: learning_rate})\n",
    "                \n",
    "        if step%log_batch_step == 0:\n",
    "            train_accuracy = sess.run(acc, feed_dict={features:batch_features, labels: batch_labels, keep_prob: 1, lr_value:learning_rate})\n",
    "            valid_accuracy = sess.run(acc, feed_dict=valid_feed_dict)\n",
    "            test_accuracy = sess.run(acc, feed_dict=test_feed_dict)\n",
    "            print(\"Steps %d, training accuracy: %g  validation accuracy: %g test accuracy: %g\"%(step, train_accuracy, valid_accuracy, test_accuracy))\n",
    "            previous_batch = batches_history[-1] if batches_history else 0\n",
    "            batches_history.append(log_batch_step + previous_batch)\n",
    "            train_acc_history.append(train_accuracy)\n",
    "            val_acc_history.append(valid_accuracy)\n",
    "            loss_history.append(loss_value)\n",
    "        \n",
    "    # Check accuracy against Test data\n",
    "    test_accuracy = sess.run(acc, feed_dict=test_feed_dict)\n",
    "    \n",
    "    print('Training completed with test accuracy : {}'.format(test_accuracy))\n",
    "    \n",
    "    return batches_history, train_acc_history, val_acc_history, test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# fine tuning hyperparameters\n",
    "jit = [False]\n",
    "models = ['Two']\n",
    "#models = ['Four']\n",
    "steps = [10000]\n",
    "batches = [64, 128, 256]\n",
    "#batches = [128]\n",
    "lr = [1e-3, 1e-4]\n",
    "#lr = [1e-2, 1e-3]\n",
    "kp = [0.5, 0.65, 0.8]\n",
    "#kp = [0.5]\n",
    "result = []\n",
    "best_acc = 0.0\n",
    "best_params = []\n",
    "for j in jit:\n",
    "    for m in models:\n",
    "        for s in steps:\n",
    "            for b in batches:\n",
    "                for l in lr:\n",
    "                    for k in kp:\n",
    "                        batch_val, train_acc, val_acc, test_acc = run_training(s,b,l,k,j,m)\n",
    "                        if test_acc > best_acc:\n",
    "                            best_acc = test_acc\n",
    "                            best_params = [s,b,l,k,m]\n",
    "                            batch_best = batch_val\n",
    "                            acc_best = [train_acc, val_acc]\n",
    "\n",
    "print('Best Validation Accuracy : {}'.format(best_acc))\n",
    "print('Best parameters: steps:{} batches:{} learning_rate:{} keep_prob:{}'.format(best_params[0],best_params[1],best_params[2],best_params[3]))\n",
    "\n",
    "acc_plot = plt.subplot()\n",
    "acc_plot.set_title('Accuracy')\n",
    "acc_plot.plot(batch_best, acc_best[0], 'g', label='Training Accuracy')\n",
    "acc_plot.plot(batch_best, acc_best[1], 'r', label='Validation Accuracy')\n",
    "acc_plot.legend(loc=4)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Placeholder definition\n",
    "import tensorflow as tf\n",
    "graph_model_multi_others = tf.Graph()\n",
    "\n",
    "with graph_model_multi_others.as_default():\n",
    "    \n",
    "    features = tf.placeholder(tf.float32, [None, 32, 32])\n",
    "    image = tf.reshape(features, [-1,32,32,1])\n",
    "    keep_prob = tf.placeholder(tf.float32)\n",
    "    lr_value = tf.placeholder(tf.float32)\n",
    "    labels = tf.placeholder(tf.float32, [None, 43])\n",
    "\n",
    "    def weight_variable(shape):\n",
    "      initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "      return tf.Variable(initial)\n",
    "\n",
    "    def bias_variable(shape):\n",
    "      initial = tf.constant(0.1, shape=shape)\n",
    "      return tf.Variable(initial)\n",
    "\n",
    "    def conv2d(x, W):\n",
    "      return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "    def max_pool_2x2(x):\n",
    "      return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n",
    "                            strides=[1, 2, 2, 1], padding='SAME')\n",
    "    # normalize\n",
    "    def norm(x):\n",
    "        return tf.nn.lrn(x, 4, bias=1.0, alpha=0.001/9.00, beta=0.75)\n",
    "    \n",
    "    def model1():\n",
    "        W_conv1 = weight_variable([5, 5, 1, 32])\n",
    "        b_conv1 = bias_variable([32])\n",
    "        h_conv1 = tf.nn.relu(conv2d(image, W_conv1) + b_conv1)\n",
    "        h_pool1 = max_pool_2x2(h_conv1)\n",
    "        h_norm1 = norm(h_pool1)\n",
    "        W_conv2 = weight_variable([5, 5, 32, 64])\n",
    "        b_conv2 = bias_variable([64])\n",
    "        h_conv2 = tf.nn.relu(conv2d(h_norm1, W_conv2) + b_conv2)\n",
    "        h_norm2 = norm(h_conv2)\n",
    "        h_pool2 = max_pool_2x2(h_norm2)\n",
    "        W_conv3 = weight_variable([3, 3, 64, 128])\n",
    "        b_conv3 = bias_variable([128])\n",
    "        h_conv3 = tf.nn.relu(conv2d(h_pool2, W_conv3) + b_conv3)\n",
    "        h_pool3 = max_pool_2x2(h_conv3)\n",
    "        h_norm3 = norm(h_pool3)\n",
    "        W_conv4 = weight_variable([1, 1, 128, 256])\n",
    "        b_conv4 = bias_variable([256])\n",
    "        h_conv4 = tf.nn.relu(conv2d(h_norm3, W_conv4) + b_conv4)\n",
    "        h_norm4 = norm(h_conv4)\n",
    "        h_pool4 = max_pool_2x2(h_norm4)\n",
    "        h_pool3_flat = tf.reshape(h_norm3, [-1, 4*4*128])\n",
    "        h_pool4_flat = tf.reshape(h_pool4, [-1, 2*2*256])\n",
    "        print(\"h_pool3_flat: \", h_pool3_flat.get_shape())\n",
    "        print(\"h_pool4_flat: \", h_pool4_flat.get_shape())\n",
    "        combined_flat = tf.concat(1, [h_pool3_flat, h_pool4_flat])\n",
    "        dim = combined_flat.get_shape()[1].value\n",
    "        print(\"combined_flat: \", combined_flat.get_shape())\n",
    "        W_fc1 = weight_variable([dim, 1024])\n",
    "        b_fc1 = bias_variable([1024])\n",
    "        h_fc1 = tf.nn.relu(tf.matmul(combined_flat, W_fc1) + b_fc1)\n",
    "        h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "        W_fc2 = weight_variable([1024, 43])\n",
    "        b_fc2 = bias_variable([43])\n",
    "        y_conv = tf.matmul(h_fc1_drop, W_fc2) + b_fc2\n",
    "        \n",
    "        return y_conv\n",
    "        \n",
    "\n",
    "    def loss(logits, labels):\n",
    "        loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, labels))\n",
    "        return loss\n",
    "\n",
    "    def training(loss, learning_rate, name = 'Adam'):\n",
    "        if name == 'GD':\n",
    "            optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "        else:\n",
    "            optimizer = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "        return optimizer\n",
    "\n",
    "    def evaluation(logits, labels):\n",
    "        correct_prediction = tf.equal(tf.argmax(logits,1), tf.argmax(labels,1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "        return accuracy\n",
    "    \n",
    "    def prediction_prob(logits):\n",
    "        return tf.nn.softmax(logits)\n",
    "    \n",
    "    def prediction_class(logits):\n",
    "        return tf.nn.top_k(logits)\n",
    "    \n",
    "    #logits, graphdef = model()\n",
    "    logits = model1()\n",
    "    losses = loss(logits,labels)\n",
    "    train_op = training(losses, lr_value)\n",
    "    acc = evaluation(logits,labels)\n",
    "    pred = prediction_prob(logits)\n",
    "    pred_class = prediction_class(logits)\n",
    "    init = tf.initialize_all_variables()\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "print('MultiScale model defined..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import time\n",
    "\n",
    "def run_training(steps,batch_size,learning_rate,k_prob,use_jitter=False, modelname ='Two'):\n",
    "    stime = time.time()\n",
    "    sess = tf.Session(graph=graph_model_multi_others)\n",
    "    sess.run(init)\n",
    "    \n",
    "    train_acc_history = []\n",
    "    val_acc_history = []\n",
    "    loss_history = []\n",
    "    log_batch_step = 1000\n",
    "    batches_history = []\n",
    "       \n",
    "    if not use_jitter:\n",
    "        norm_train_features = preprocess_images(train_features)\n",
    "    else:\n",
    "        norm_train_features = train_features\n",
    "    \n",
    "    norm_valid_features = preprocess_images(valid_features)\n",
    "    norn_test_features = preprocess_images(X_test[1:1000])\n",
    "    \n",
    "    valid_feed_dict = {features: norm_valid_features, labels : valid_labels, keep_prob: k_prob, lr_value: learning_rate}\n",
    "    test_feed_dict = {features: norn_test_features, labels : test_labels[1:1000], keep_prob: 1, lr_value: learning_rate}\n",
    "    \n",
    "    print('Training model with {} layers started for step: {} batchsize: {} learning_rate: {} keep_prob: {}'.format(modelname, steps,batch_size,learning_rate,k_prob))\n",
    "    \n",
    "    for step in range(steps):\n",
    "        # Get a batch of training features and labels\n",
    "        #batch_start = batch_i*batch_size\n",
    "        #batch_features = train_features[batch_start:batch_start + batch_size]\n",
    "        #batch_labels = train_labels[batch_start:batch_start + batch_size]\n",
    "        batch_start = np.random.choice(norm_train_features.shape[0],batch_size)\n",
    "        batch_features = norm_train_features[batch_start]\n",
    "        batch_labels = train_labels[batch_start]\n",
    "        if use_jitter:\n",
    "            batch_features, batch_labels = jitter_image_data(batch_features, batch_labels, batch_size)\n",
    "\n",
    "        # Run optimizer\n",
    "        loss_value = sess.run(train_op, feed_dict={features: batch_features, labels: batch_labels, keep_prob: k_prob, lr_value: learning_rate})\n",
    "                \n",
    "        if step%log_batch_step == 0:\n",
    "            train_accuracy = sess.run(acc, feed_dict={features:batch_features, labels: batch_labels, keep_prob: 1, lr_value:learning_rate})\n",
    "            valid_accuracy = sess.run(acc, feed_dict=valid_feed_dict)\n",
    "            test_accuracy = sess.run(acc, feed_dict=test_feed_dict)\n",
    "            print(\"Steps %d, training accuracy: %g  validation accuracy: %g test accuracy: %g\"%(step, train_accuracy, valid_accuracy, test_accuracy))\n",
    "            previous_batch = batches_history[-1] if batches_history else 0\n",
    "            batches_history.append(log_batch_step + previous_batch)\n",
    "            train_acc_history.append(train_accuracy)\n",
    "            val_acc_history.append(valid_accuracy)\n",
    "            loss_history.append(loss_value)\n",
    "        \n",
    "    # Check accuracy against Test data\n",
    "    test_accuracy = sess.run(acc, feed_dict=test_feed_dict)\n",
    "    \n",
    "    print('Training completed with test accuracy : {}'.format(test_accuracy))\n",
    "    \n",
    "    return batches_history, train_acc_history, val_acc_history, test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# fine tuning hyperparameters\n",
    "jit = [False]\n",
    "models = ['Two']\n",
    "#models = ['Four']\n",
    "steps = [10000]\n",
    "batches = [32, 64, 128]\n",
    "#batches = [128]\n",
    "#lr = [1e-2, 1e-3, 1e-4]\n",
    "lr = [1e-2, 1e-3]\n",
    "kp = [0.5, 0.65, 0.8]\n",
    "#kp = [0.5]\n",
    "result = []\n",
    "best_acc = 0.0\n",
    "best_params = []\n",
    "for j in jit:\n",
    "    for m in models:\n",
    "        for s in steps:\n",
    "            for b in batches:\n",
    "                for l in lr:\n",
    "                    for k in kp:\n",
    "                        batch_val, train_acc, val_acc, test_acc = run_training(s,b,l,k,j,m)\n",
    "                        if test_acc > best_acc:\n",
    "                            best_acc = test_acc\n",
    "                            best_params = [s,b,l,k,m]\n",
    "                            batch_best = batch_val\n",
    "                            acc_best = [train_acc, val_acc]\n",
    "\n",
    "print('Best Validation Accuracy : {}'.format(best_acc))\n",
    "print('Best parameters: steps:{} batches:{} learning_rate:{} keep_prob:{}'.format(best_params[0],best_params[1],best_params[2],best_params[3]))\n",
    "\n",
    "acc_plot = plt.subplot()\n",
    "acc_plot.set_title('Accuracy')\n",
    "acc_plot.plot(batch_best, acc_best[0], 'g', label='Training Accuracy')\n",
    "acc_plot.plot(batch_best, acc_best[1], 'r', label='Validation Accuracy')\n",
    "acc_plot.legend(loc=4)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultiScale model2 defined..\n"
     ]
    }
   ],
   "source": [
    "#Placeholder definition\n",
    "import tensorflow as tf\n",
    "graph_model_multi2 = tf.Graph()\n",
    "\n",
    "with graph_model_multi2.as_default():\n",
    "    \n",
    "    features = tf.placeholder(tf.float32, [None, 32, 32])\n",
    "    image = tf.reshape(features, [-1,32,32,1])\n",
    "    keep_prob = tf.placeholder(tf.float32)\n",
    "    lr_value = tf.placeholder(tf.float32)\n",
    "    labels = tf.placeholder(tf.float32, [None, 43])\n",
    "\n",
    "    def weight_variable(shape):\n",
    "      initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "      return tf.Variable(initial)\n",
    "\n",
    "    def bias_variable(shape):\n",
    "      initial = tf.constant(0.1, shape=shape)\n",
    "      return tf.Variable(initial)\n",
    "\n",
    "    def conv2d(x, W):\n",
    "      return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "    def max_pool_2x2(x):\n",
    "      return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n",
    "                            strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "    def model():\n",
    "        endpoints = {}\n",
    "        with tf.variable_scope('conv1') as scope:\n",
    "            W_conv1 = weight_variable([5, 5, 1, 32])\n",
    "            b_conv1 = bias_variable([32])\n",
    "            h_conv1 = tf.nn.relu(conv2d(image, W_conv1) + b_conv1)\n",
    "            h_pool1 = max_pool_2x2(h_conv1)\n",
    "            endpoints['conv1'] = h_conv1\n",
    "            endpoints['conv1_pool1'] = h_pool1\n",
    "\n",
    "        with tf.variable_scope('conv2') as scope:\n",
    "            W_conv2 = weight_variable([5, 5, 32, 64])\n",
    "            b_conv2 = bias_variable([64])\n",
    "            h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "            h_pool2 = max_pool_2x2(h_conv2)\n",
    "            endpoints['conv2'] = h_conv2\n",
    "            endpoints['conv2_pool2'] = h_pool2\n",
    "        \n",
    "        with tf.variable_scope('combined') as scope:\n",
    "            h_pool1_flat = tf.reshape(h_pool1, [-1, 16*16*32])\n",
    "            h_pool2_flat = tf.reshape(h_pool2, [-1, 8*8*64])\n",
    "            combined_pool_flat = tf.concat(1, [h_pool1_flat, h_pool2_flat])\n",
    "            combined_pool_flat_shape = combined_pool_flat.get_shape()[1].value\n",
    "            endpoints['combined'] = combined_pool_flat\n",
    "\n",
    "        with tf.variable_scope('fc1') as scope:\n",
    "            W_fc1 = weight_variable([combined_pool_flat_shape, 1024])\n",
    "            b_fc1 = bias_variable([1024])\n",
    "            h_fc1 = tf.nn.relu(tf.matmul(combined_pool_flat, W_fc1) + b_fc1)\n",
    "            h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "            endpoints['fc1'] = h_fc1_drop\n",
    "        \n",
    "        with tf.variable_scope('fc2') as scope:\n",
    "            W_fc2 = weight_variable([1024, 100])\n",
    "            b_fc2 = bias_variable([100])\n",
    "            h_fc2 = tf.nn.relu(tf.matmul(h_fc1_drop, W_fc2) + b_fc2)\n",
    "            h_fc2_drop = tf.nn.dropout(h_fc2, keep_prob)\n",
    "            endpoints['fc2'] = h_fc2_drop\n",
    "            \n",
    "        with tf.variable_scope('fc3') as scope:\n",
    "            W_fc3 = weight_variable([100, 43])\n",
    "            b_fc3 = bias_variable([43])\n",
    "            logits = tf.matmul(h_fc2_drop, W_fc3) + b_fc3\n",
    "            endpoints['logits'] = logits\n",
    "\n",
    "        return logits, endpoints\n",
    "\n",
    "    def loss(logits, labels):\n",
    "        loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, labels))\n",
    "        return loss\n",
    "\n",
    "    def training(loss, learning_rate, name = 'Adam'):\n",
    "        if name == 'GD':\n",
    "            optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "        else:\n",
    "            optimizer = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "        return optimizer\n",
    "\n",
    "    def evaluation(logits, labels):\n",
    "        correct_prediction = tf.equal(tf.argmax(logits,1), tf.argmax(labels,1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "        return accuracy\n",
    "    \n",
    "    def prediction_prob(logits):\n",
    "        return tf.nn.softmax(logits)\n",
    "    \n",
    "    def prediction_class(logits):\n",
    "        return tf.nn.top_k(logits)\n",
    "    \n",
    "    logits, _ = model()\n",
    "    losses = loss(logits,labels)\n",
    "    train_op = training(losses, lr_value)\n",
    "    acc = evaluation(logits,labels)\n",
    "    pred = prediction_prob(logits)\n",
    "    pred_class = prediction_class(logits)\n",
    "    init = tf.initialize_all_variables()\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "print('MultiScale model2 defined..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import time\n",
    "\n",
    "def run_training(steps,batch_size,learning_rate,k_prob,use_jitter=False, modelname ='Two'):\n",
    "    stime = time.time()\n",
    "    sess = tf.Session(graph=graph_model_multi2)\n",
    "    sess.run(init)\n",
    "    \n",
    "    train_acc_history = []\n",
    "    val_acc_history = []\n",
    "    loss_history = []\n",
    "    log_batch_step = 1000\n",
    "    batches_history = []\n",
    "       \n",
    "    if not use_jitter:\n",
    "        norm_train_features = preprocess_images(train_features)\n",
    "    else:\n",
    "        norm_train_features = train_features\n",
    "    \n",
    "    norm_valid_features = preprocess_images(valid_features)\n",
    "    norn_test_features = preprocess_images(X_test[1:1000])\n",
    "    \n",
    "    valid_feed_dict = {features: norm_valid_features, labels : valid_labels, keep_prob: k_prob}\n",
    "    test_feed_dict = {features: norn_test_features, labels : test_labels[1:1000], keep_prob: 1}\n",
    "    \n",
    "    print('Training model with {} layers started for step: {} batchsize: {} learning_rate: {} keep_prob: {}'.format(modelname, steps,batch_size,learning_rate,k_prob))\n",
    "    \n",
    "    for step in range(steps):\n",
    "        # Get a batch of training features and labels\n",
    "        #batch_start = batch_i*batch_size\n",
    "        #batch_features = train_features[batch_start:batch_start + batch_size]\n",
    "        #batch_labels = train_labels[batch_start:batch_start + batch_size]\n",
    "        batch_start = np.random.choice(norm_train_features.shape[0],batch_size)\n",
    "        batch_features = norm_train_features[batch_start]\n",
    "        batch_labels = train_labels[batch_start]\n",
    "        if use_jitter:\n",
    "            batch_features, batch_labels = jitter_image_data(batch_features, batch_labels, batch_size)\n",
    "\n",
    "        # Run optimizer\n",
    "        loss_value = sess.run(train_op, feed_dict={features: batch_features, labels: batch_labels, keep_prob: k_prob})\n",
    "                \n",
    "        if step%log_batch_step == 0:\n",
    "            train_accuracy = acc.eval(feed_dict={features:batch_features, labels: batch_labels, keep_prob: 1})\n",
    "            valid_accuracy = acc.eval(feed_dict=valid_feed_dict)\n",
    "            test_accuracy = acc.eval(feed_dict=test_feed_dict)\n",
    "            print(\"Steps %d, training accuracy: %g  validation accuracy: %g test accuracy: %g\"%(step, train_accuracy, valid_accuracy, test_accuracy))\n",
    "            previous_batch = batches_history[-1] if batches_history else 0\n",
    "            batches_history.append(log_batch_step + previous_batch)\n",
    "            train_acc_history.append(train_accuracy)\n",
    "            val_acc_history.append(valid_accuracy)\n",
    "            loss_history.append(loss_value)\n",
    "        \n",
    "    # Check accuracy against Test data\n",
    "    test_accuracy = sess.run(acc, feed_dict=test_feed_dict)\n",
    "    \n",
    "    print('Training completed with test accuracy : {}'.format(test_accuracy))\n",
    "    \n",
    "    return batches_history, train_acc_history, val_acc_history, test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# fine tuning hyperparameters\n",
    "jit = [False]\n",
    "models = ['Two']\n",
    "#models = ['Four']\n",
    "steps = [10000]\n",
    "batches = [64, 128, 256]\n",
    "#batches = [128]\n",
    "#lr = [1e-2, 1e-3, 1e-4]\n",
    "lr = [1e-2, 1e-3]\n",
    "kp = [0.5, 0.65, 0.8]\n",
    "kp = [0.5]\n",
    "result = []\n",
    "best_acc = 0.0\n",
    "best_params = []\n",
    "for j in jit:\n",
    "    for m in models:\n",
    "        for s in steps:\n",
    "            for b in batches:\n",
    "                for l in lr:\n",
    "                    for k in kp:\n",
    "                        batch_val, train_acc, val_acc, test_acc = run_training(s,b,l,k,j,m)\n",
    "                        if test_acc > best_acc:\n",
    "                            best_acc = test_acc\n",
    "                            best_params = [s,b,l,k,m]\n",
    "                            batch_best = batch_val\n",
    "                            acc_best = [train_acc, val_acc]\n",
    "\n",
    "print('Best Validation Accuracy : {}'.format(best_acc))\n",
    "print('Best parameters: steps:{} batches:{} learning_rate:{} keep_prob:{}'.format(best_params[0],best_params[1],best_params[2],best_params[3]))\n",
    "\n",
    "acc_plot = plt.subplot()\n",
    "acc_plot.set_title('Accuracy')\n",
    "acc_plot.plot(batch_best, acc_best[0], 'g', label='Training Accuracy')\n",
    "acc_plot.plot(batch_best, acc_best[1], 'r', label='Validation Accuracy')\n",
    "acc_plot.legend(loc=4)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Placeholder definition\n",
    "import tensorflow as tf\n",
    "graph_model_simple = tf.Graph()\n",
    "\n",
    "with graph_model_simple.as_default():\n",
    "    \n",
    "    features = tf.placeholder(tf.float32, [None, 32, 32])\n",
    "    image = tf.reshape(features, [-1,32,32,1])\n",
    "    keep_prob = tf.placeholder(tf.float32)\n",
    "    labels = tf.placeholder(tf.float32, [None, 43])\n",
    "\n",
    "    def weight_variable(shape):\n",
    "      initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "      return tf.Variable(initial)\n",
    "\n",
    "    def bias_variable(shape):\n",
    "      initial = tf.constant(0.1, shape=shape)\n",
    "      return tf.Variable(initial)\n",
    "\n",
    "    def conv2d(x, W):\n",
    "      return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "    def max_pool_2x2(x):\n",
    "      return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n",
    "                            strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "    def model_twolayers():\n",
    "        endpoints = {}\n",
    "        with tf.variable_scope('conv1') as scope:\n",
    "            W_conv1 = weight_variable([5, 5, 1, 32])\n",
    "            b_conv1 = bias_variable([32])\n",
    "            h_conv1 = tf.nn.relu(conv2d(image, W_conv1) + b_conv1)\n",
    "            h_pool1 = max_pool_2x2(h_conv1)\n",
    "            endpoints['conv1'] = h_conv1\n",
    "            endpoints['conv1_pool1'] = h_pool1\n",
    "\n",
    "        with tf.variable_scope('conv2') as scope:\n",
    "            W_conv2 = weight_variable([5, 5, 32, 64])\n",
    "            b_conv2 = bias_variable([64])\n",
    "            h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "            h_pool2 = max_pool_2x2(h_conv2)\n",
    "            endpoints['conv2'] = h_conv2\n",
    "            endpoints['conv2_pool2'] = h_pool2\n",
    "\n",
    "        with tf.variable_scope('fc1') as scope:\n",
    "            W_fc1 = weight_variable([8*8*64, 1024])\n",
    "            b_fc1 = bias_variable([1024])\n",
    "            h_pool2_flat = tf.reshape(h_pool2, [-1, 8*8*64])\n",
    "            h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n",
    "            h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "            endpoints['fc1'] = h_fc1_drop\n",
    "\n",
    "        with tf.variable_scope('fc2') as scope:\n",
    "            W_fc2 = weight_variable([1024, 43])\n",
    "            b_fc2 = bias_variable([43])\n",
    "            logits = tf.matmul(h_fc1_drop, W_fc2) + b_fc2\n",
    "            endpoints['logits'] = logits\n",
    "\n",
    "        return logits, endpoints\n",
    "\n",
    "    def model_fourlayers():\n",
    "        endpoints = {}\n",
    "        with tf.variable_scope('conv1') as scope:\n",
    "            W_conv1 = weight_variable([5, 5, 1, 32])\n",
    "            b_conv1 = bias_variable([32])\n",
    "            h_conv1 = tf.nn.relu(conv2d(image, W_conv1) + b_conv1)\n",
    "            h_pool1 = max_pool_2x2(h_conv1)\n",
    "            endpoints['conv1'] = h_conv1\n",
    "            endpoints['conv1_pool1'] = h_pool1\n",
    "\n",
    "        with tf.variable_scope('conv2') as scope:\n",
    "            W_conv2 = weight_variable([5, 5, 32, 64])\n",
    "            b_conv2 = bias_variable([64])\n",
    "            h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "            h_pool2 = max_pool_2x2(h_conv2)\n",
    "            endpoints['conv2'] = h_conv2\n",
    "            endpoints['conv2_pool2'] = h_pool2\n",
    "\n",
    "        with tf.variable_scope('conv3') as scope:\n",
    "            W_conv3 = weight_variable([5, 5, 64, 128])\n",
    "            b_conv3 = bias_variable([128])\n",
    "            h_conv3 = tf.nn.relu(conv2d(h_pool2, W_conv3) + b_conv3)\n",
    "            h_pool3 = max_pool_2x2(h_conv3)\n",
    "            endpoints['conv3'] = h_conv3\n",
    "            endpoints['conv3_pool3'] = h_pool3\n",
    "\n",
    "        with tf.variable_scope('conv4') as scope:\n",
    "            W_conv4 = weight_variable([3, 3, 128, 256])\n",
    "            b_conv4 = bias_variable([256])\n",
    "            h_conv4 = tf.nn.relu(conv2d(h_pool3, W_conv4) + b_conv4)\n",
    "            h_pool4 = max_pool_2x2(h_conv4)\n",
    "            endpoints['conv4'] = h_conv4\n",
    "            endpoints['conv4_pool4'] = h_pool4\n",
    "\n",
    "        with tf.variable_scope('fc1') as scope:\n",
    "            W_fc1 = weight_variable([2*2*256, 512])\n",
    "            b_fc1 = bias_variable([512])\n",
    "            h_pool4_flat = tf.reshape(h_pool4, [-1, 2*2*256])\n",
    "            h_fc1 = tf.nn.relu(tf.matmul(h_pool4_flat, W_fc1) + b_fc1)\n",
    "            h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "            endpoints['fc1'] = h_fc1_drop\n",
    "\n",
    "        with tf.variable_scope('fc2') as scope:\n",
    "            W_fc2 = weight_variable([512, 43])\n",
    "            b_fc2 = bias_variable([43])\n",
    "            logits = tf.matmul(h_fc1_drop, W_fc2) + b_fc2\n",
    "            endpoints['logits'] = logits\n",
    "\n",
    "        return logits, endpoints\n",
    "\n",
    "    def loss(logits, labels):\n",
    "        loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, labels))\n",
    "        return loss\n",
    "\n",
    "    def training(loss, learning_rate, name = 'Adam'):\n",
    "        if name == 'GD':\n",
    "            optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "        else:\n",
    "            optimizer = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "        return optimizer\n",
    "\n",
    "    def evaluation(logits, labels):\n",
    "        correct_prediction = tf.equal(tf.argmax(logits,1), tf.argmax(labels,1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "        return accuracy\n",
    "    \n",
    "    def prediction_prob(logits):\n",
    "        return tf.nn.softmax(logits)\n",
    "    \n",
    "    def prediction_class(logits):\n",
    "        return tf.nn.top_k(logits)\n",
    "    \n",
    "    logits, _ = model_twolayers()\n",
    "    losses = loss(logits,labels)\n",
    "    train_op = training(losses, 0.001)\n",
    "    acc = evaluation(logits,labels)\n",
    "    pred = prediction_prob(logits)\n",
    "    pred_class = prediction_class(logits)\n",
    "    init = tf.initialize_all_variables()\n",
    "    saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "curr_path = os.getcwd()\n",
    "model_path = curr_path +'/models/model_simple_jitter.ckpt'\n",
    "\n",
    "train = True\n",
    "use_jitter = True\n",
    "steps = 100000\n",
    "batch_size = 128\n",
    "k_prob = 0.6\n",
    "learning_rate = 0.001\n",
    "# Data preparation \n",
    "if not use_jitter:\n",
    "    norm_train_features = preprocess_images(train_features)\n",
    "else:\n",
    "    norm_train_features = train_features\n",
    "    \n",
    "norm_valid_features = preprocess_images(valid_features)\n",
    "norm_test_features = preprocess_images(X_test)\n",
    "n_test = norm_test_features.shape[0]\n",
    "    \n",
    "valid_feed_dict = {features: norm_valid_features, labels : valid_labels, keep_prob: k_prob}\n",
    "\n",
    "#sess = tf.Session(graph=graph_model_simple)\n",
    "#sess.run(init)\n",
    "with tf.Session(graph=graph_model_simple) as sess:\n",
    "    if train:\n",
    "        sess.run(init)\n",
    "        for step in range(steps):\n",
    "            batch_start = np.random.choice(norm_train_features.shape[0],batch_size)\n",
    "            batch_features = norm_train_features[batch_start]\n",
    "            batch_labels = train_labels[batch_start]\n",
    "\n",
    "            if use_jitter:\n",
    "                batch_features, batch_labels = jitter_image_data(batch_features, batch_labels, batch_size)\n",
    "                # Run optimizer\n",
    "            loss_value = sess.run(train_op, feed_dict={features: batch_features, labels: batch_labels, keep_prob: k_prob})\n",
    "\n",
    "            if step%1000 == 0:\n",
    "                train_accuracy = sess.run([acc], feed_dict={features:batch_features, labels: batch_labels, keep_prob: 1})\n",
    "                valid_accuracy = sess.run([acc], feed_dict=valid_feed_dict)\n",
    "                batch_count = int(math.ceil(n_test/batch_size))\n",
    "                total = 0\n",
    "                for i in range(batch_count):\n",
    "                    batch_start = i*batch_size\n",
    "                    test_batch_features = norm_test_features[batch_start:batch_start + batch_size]\n",
    "                    test_batch_labels = test_labels[batch_start:batch_start + batch_size]\n",
    "                    total += sess.run(acc, feed_dict={features:test_batch_features, labels: test_batch_labels, keep_prob: 1})\n",
    "                    #print(total)\n",
    "                #print(total,n_test)\n",
    "                test_accuracy = total / batch_count\n",
    "                #print(\"Steps %d, training accuracy: %g  validation accuracy: %g test accuracy: %g\"%(step, train_accuracy, valid_accuracy, test_accuracy))\n",
    "                print(\"Steps {}, training accuracy: {}  validation accuracy: {} test accuracy: {}\".format(step, train_accuracy, valid_accuracy, test_accuracy))\n",
    "\n",
    "            if ((step == (steps-1)) or (test_accuracy > 0.98)):\n",
    "                batch_count = int(math.ceil(n_test/batch_size))\n",
    "                total = 0\n",
    "                for i in range(batch_count):\n",
    "                    batch_start = i*batch_size\n",
    "                    test_batch_features = norm_test_features[batch_start:batch_start + batch_size]\n",
    "                    test_batch_labels = test_labels[batch_start:batch_start + batch_size]\n",
    "                    total += sess.run(acc, feed_dict={features:test_batch_features, labels: test_batch_labels, keep_prob: 1})\n",
    "                test_accuracy = total / batch_count\n",
    "                print('Final test accuracy: {}'.format(test_accuracy))\n",
    "                save_path = saver.save(sess,model_path)\n",
    "                print(\"Model saved.\")\n",
    "    else:\n",
    "        # Here's where you're restoring the variables w and b.\n",
    "        # Note that the graph is exactly as it was when the variables were\n",
    "        # saved in a prior training run.\n",
    "        #ckpt = tf.train.get_checkpoint_state(check_dir)\n",
    "        #if ckpt and ckpt.model_checkpoint_path:\n",
    "        #saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "        #print('Model Restored..')\n",
    "        saver.restore(sess,model_path)\n",
    "        batch_count = int(math.ceil(n_test/batch_size))\n",
    "        total = 0\n",
    "        for i in range(batch_count):\n",
    "            batch_start = i*batch_size\n",
    "            test_batch_features = norm_test_features[batch_start:batch_start + batch_size]\n",
    "            test_batch_labels = test_labels[batch_start:batch_start + batch_size]\n",
    "            total += sess.run(acc, feed_dict={features:test_batch_features, labels: test_batch_labels, keep_prob: 1})\n",
    "        test_accuracy = total / batch_count\n",
    "        print('Restored test accuracy: {}'.format(test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "curr_path = os.getcwd()\n",
    "model_path = curr_path +'/models/model_simple_jitter2.ckpt'\n",
    "\n",
    "train = True\n",
    "use_jitter = True\n",
    "steps = 100000\n",
    "batch_size = 64\n",
    "k_prob = 0.6\n",
    "learning_rate = 0.0001\n",
    "# Data preparation \n",
    "if not use_jitter:\n",
    "    norm_train_features = preprocess_images(train_features)\n",
    "else:\n",
    "    norm_train_features = train_features\n",
    "    \n",
    "norm_valid_features = preprocess_images(valid_features)\n",
    "norm_test_features = preprocess_images(X_test)\n",
    "n_test = norm_test_features.shape[0]\n",
    "    \n",
    "valid_feed_dict = {features: norm_valid_features, labels : valid_labels, keep_prob: k_prob}\n",
    "\n",
    "#sess = tf.Session(graph=graph_model_simple)\n",
    "#sess.run(init)\n",
    "with tf.Session(graph=graph_model_simple) as sess:\n",
    "    if train:\n",
    "        sess.run(init)\n",
    "        for step in range(steps):\n",
    "            batch_start = np.random.choice(norm_train_features.shape[0],batch_size)\n",
    "            batch_features = norm_train_features[batch_start]\n",
    "            batch_labels = train_labels[batch_start]\n",
    "\n",
    "            if use_jitter:\n",
    "                batch_features, batch_labels = jitter_image_data(batch_features, batch_labels, batch_size)\n",
    "                # Run optimizer\n",
    "            loss_value = sess.run(train_op, feed_dict={features: batch_features, labels: batch_labels, keep_prob: k_prob})\n",
    "\n",
    "            if step%1000 == 0:\n",
    "                train_accuracy = sess.run([acc], feed_dict={features:batch_features, labels: batch_labels, keep_prob: 1})\n",
    "                valid_accuracy = sess.run([acc], feed_dict=valid_feed_dict)\n",
    "                batch_count = int(math.ceil(n_test/batch_size))\n",
    "                total = 0\n",
    "                for i in range(batch_count):\n",
    "                    batch_start = i*batch_size\n",
    "                    test_batch_features = norm_test_features[batch_start:batch_start + batch_size]\n",
    "                    test_batch_labels = test_labels[batch_start:batch_start + batch_size]\n",
    "                    total += sess.run(acc, feed_dict={features:test_batch_features, labels: test_batch_labels, keep_prob: 1})\n",
    "                    #print(total)\n",
    "                #print(total,n_test)\n",
    "                test_accuracy = total / batch_count\n",
    "                #print(\"Steps %d, training accuracy: %g  validation accuracy: %g test accuracy: %g\"%(step, train_accuracy, valid_accuracy, test_accuracy))\n",
    "                print(\"Steps {}, training accuracy: {}  validation accuracy: {} test accuracy: {}\".format(step, train_accuracy, valid_accuracy, test_accuracy))\n",
    "\n",
    "            if ((step == (steps-1)) or (test_accuracy > 0.98)):\n",
    "                batch_count = int(math.ceil(n_test/batch_size))\n",
    "                total = 0\n",
    "                for i in range(batch_count):\n",
    "                    batch_start = i*batch_size\n",
    "                    test_batch_features = norm_test_features[batch_start:batch_start + batch_size]\n",
    "                    test_batch_labels = test_labels[batch_start:batch_start + batch_size]\n",
    "                    total += sess.run(acc, feed_dict={features:test_batch_features, labels: test_batch_labels, keep_prob: 1})\n",
    "                test_accuracy = total / batch_count\n",
    "                print('Final test accuracy: {}'.format(test_accuracy))\n",
    "                save_path = saver.save(sess,model_path)\n",
    "                print(\"Model saved.\")\n",
    "    else:\n",
    "        # Here's where you're restoring the variables w and b.\n",
    "        # Note that the graph is exactly as it was when the variables were\n",
    "        # saved in a prior training run.\n",
    "        #ckpt = tf.train.get_checkpoint_state(check_dir)\n",
    "        #if ckpt and ckpt.model_checkpoint_path:\n",
    "        #saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "        #print('Model Restored..')\n",
    "        saver.restore(sess,model_path)\n",
    "        batch_count = int(math.ceil(n_test/batch_size))\n",
    "        total = 0\n",
    "        for i in range(batch_count):\n",
    "            batch_start = i*batch_size\n",
    "            test_batch_features = norm_test_features[batch_start:batch_start + batch_size]\n",
    "            test_batch_labels = test_labels[batch_start:batch_start + batch_size]\n",
    "            total += sess.run(acc, feed_dict={features:test_batch_features, labels: test_batch_labels, keep_prob: 1})\n",
    "        test_accuracy = total / batch_count\n",
    "        print('Restored test accuracy: {}'.format(test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "curr_path = os.getcwd()\n",
    "model_path = curr_path +'/models/model_multi_1.ckpt'\n",
    "\n",
    "train = True\n",
    "use_jitter = False\n",
    "steps = 100000\n",
    "batch_size = 128\n",
    "k_prob = 0.5\n",
    "learning_rate = 0.001\n",
    "# Data preparation \n",
    "if not use_jitter:\n",
    "    norm_train_features = preprocess_images(train_features)\n",
    "else:\n",
    "    norm_train_features = train_features\n",
    "    \n",
    "norm_valid_features = preprocess_images(valid_features)\n",
    "norm_test_features = preprocess_images(X_test)\n",
    "n_test = norm_test_features.shape[0]\n",
    "    \n",
    "valid_feed_dict = {features: norm_valid_features, labels : valid_labels, keep_prob: k_prob, lr_value: learning_rate}\n",
    "\n",
    "#sess = tf.Session(graph=graph_model_simple)\n",
    "#sess.run(init)\n",
    "with tf.Session(graph=graph_model_multi3) as sess:\n",
    "    if train:\n",
    "        sess.run(init)\n",
    "        for step in range(steps):\n",
    "            batch_start = np.random.choice(norm_train_features.shape[0],batch_size)\n",
    "            batch_features = norm_train_features[batch_start]\n",
    "            batch_labels = train_labels[batch_start]\n",
    "\n",
    "            if use_jitter:\n",
    "                batch_features, batch_labels = jitter_image_data(batch_features, batch_labels, batch_size)\n",
    "                # Run optimizer\n",
    "            loss_value = sess.run(train_op, feed_dict={features: batch_features, labels: batch_labels, keep_prob: k_prob, lr_value: learning_rate})\n",
    "\n",
    "            if step%1000 == 0:\n",
    "                train_accuracy = sess.run([acc], feed_dict={features:batch_features, labels: batch_labels, keep_prob: 1, lr_value: learning_rate})\n",
    "                valid_accuracy = sess.run([acc], feed_dict=valid_feed_dict)\n",
    "                batch_count = int(math.ceil(n_test/batch_size))\n",
    "                total = 0\n",
    "                for i in range(batch_count):\n",
    "                    batch_start = i*batch_size\n",
    "                    test_batch_features = norm_test_features[batch_start:batch_start + batch_size]\n",
    "                    test_batch_labels = test_labels[batch_start:batch_start + batch_size]\n",
    "                    total += sess.run(acc, feed_dict={features:test_batch_features, labels: test_batch_labels, keep_prob: 1, lr_value: learning_rate})\n",
    "                    #print(total)\n",
    "                #print(total,n_test)\n",
    "                test_accuracy = total / batch_count\n",
    "                #print(\"Steps %d, training accuracy: %g  validation accuracy: %g test accuracy: %g\"%(step, train_accuracy, valid_accuracy, test_accuracy))\n",
    "                print(\"Steps {}, training accuracy: {}  validation accuracy: {} test accuracy: {}\".format(step, train_accuracy, valid_accuracy, test_accuracy))\n",
    "\n",
    "            if ((step == (steps-1)) or (test_accuracy > 0.98)):\n",
    "                batch_count = int(math.ceil(n_test/batch_size))\n",
    "                total = 0\n",
    "                for i in range(batch_count):\n",
    "                    batch_start = i*batch_size\n",
    "                    test_batch_features = norm_test_features[batch_start:batch_start + batch_size]\n",
    "                    test_batch_labels = test_labels[batch_start:batch_start + batch_size]\n",
    "                    total += sess.run(acc, feed_dict={features:test_batch_features, labels: test_batch_labels, keep_prob: 1, lr_value:learning_rate})\n",
    "                test_accuracy = total / batch_count\n",
    "                print('Final test accuracy: {}'.format(test_accuracy))\n",
    "                save_path = saver.save(sess,model_path)\n",
    "                print(\"Model saved.\")\n",
    "    else:\n",
    "        # Here's where you're restoring the variables w and b.\n",
    "        # Note that the graph is exactly as it was when the variables were\n",
    "        # saved in a prior training run.\n",
    "        #ckpt = tf.train.get_checkpoint_state(check_dir)\n",
    "        #if ckpt and ckpt.model_checkpoint_path:\n",
    "        #saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "        #print('Model Restored..')\n",
    "        saver.restore(sess,model_path)\n",
    "        batch_count = int(math.ceil(n_test/batch_size))\n",
    "        total = 0\n",
    "        for i in range(batch_count):\n",
    "            batch_start = i*batch_size\n",
    "            test_batch_features = norm_test_features[batch_start:batch_start + batch_size]\n",
    "            test_batch_labels = test_labels[batch_start:batch_start + batch_size]\n",
    "            total += sess.run(acc, feed_dict={features:test_batch_features, labels: test_batch_labels, keep_prob: 1})\n",
    "        test_accuracy = total / batch_count\n",
    "        print('Restored test accuracy: {}'.format(test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "curr_path = os.getcwd()\n",
    "model_path = curr_path +'/models/model_multi_jitter.ckpt'\n",
    "\n",
    "train = True\n",
    "use_jitter = True\n",
    "steps = 100000\n",
    "batch_size = 128\n",
    "k_prob = 0.65\n",
    "learning_rate = 0.001\n",
    "# Data preparation \n",
    "if not use_jitter:\n",
    "    norm_train_features = preprocess_images(train_features)\n",
    "else:\n",
    "    norm_train_features = train_features\n",
    "    \n",
    "norm_valid_features = preprocess_images(valid_features)\n",
    "norm_test_features = preprocess_images(X_test)\n",
    "n_test = norm_test_features.shape[0]\n",
    "    \n",
    "valid_feed_dict = {features: norm_valid_features, labels : valid_labels, keep_prob: k_prob, lr_value: learning_rate}\n",
    "\n",
    "#sess = tf.Session(graph=graph_model_simple)\n",
    "#sess.run(init)\n",
    "with tf.Session(graph=graph_model_multi) as sess:\n",
    "    if train:\n",
    "        sess.run(init)\n",
    "        for step in range(steps):\n",
    "            batch_start = np.random.choice(norm_train_features.shape[0],batch_size)\n",
    "            batch_features = norm_train_features[batch_start]\n",
    "            batch_labels = train_labels[batch_start]\n",
    "\n",
    "            if use_jitter:\n",
    "                batch_features, batch_labels = jitter_image_data(batch_features, batch_labels, batch_size)\n",
    "                # Run optimizer\n",
    "            loss_value = sess.run(train_op, feed_dict={features: batch_features, labels: batch_labels, keep_prob: k_prob, lr_value: learning_rate})\n",
    "\n",
    "            if step%1000 == 0:\n",
    "                train_accuracy = sess.run([acc], feed_dict={features:batch_features, labels: batch_labels, keep_prob: 1, lr_value: learning_rate})\n",
    "                valid_accuracy = sess.run([acc], feed_dict=valid_feed_dict)\n",
    "                batch_count = int(math.ceil(n_test/batch_size))\n",
    "                total = 0\n",
    "                for i in range(batch_count):\n",
    "                    batch_start = i*batch_size\n",
    "                    test_batch_features = norm_test_features[batch_start:batch_start + batch_size]\n",
    "                    test_batch_labels = test_labels[batch_start:batch_start + batch_size]\n",
    "                    total += sess.run(acc, feed_dict={features:test_batch_features, labels: test_batch_labels, keep_prob: 1, lr_value: learning_rate})\n",
    "                    #print(total)\n",
    "                #print(total,n_test)\n",
    "                test_accuracy = total / batch_count\n",
    "                #print(\"Steps %d, training accuracy: %g  validation accuracy: %g test accuracy: %g\"%(step, train_accuracy, valid_accuracy, test_accuracy))\n",
    "                print(\"Steps {}, training accuracy: {}  validation accuracy: {} test accuracy: {}\".format(step, train_accuracy, valid_accuracy, test_accuracy))\n",
    "\n",
    "            if ((step == (steps-1)) or (test_accuracy > 0.98)):\n",
    "                batch_count = int(math.ceil(n_test/batch_size))\n",
    "                total = 0\n",
    "                for i in range(batch_count):\n",
    "                    batch_start = i*batch_size\n",
    "                    test_batch_features = norm_test_features[batch_start:batch_start + batch_size]\n",
    "                    test_batch_labels = test_labels[batch_start:batch_start + batch_size]\n",
    "                    total += sess.run(acc, feed_dict={features:test_batch_features, labels: test_batch_labels, keep_prob: 1, lr_value:learning_rate})\n",
    "                test_accuracy = total / batch_count\n",
    "                print('Final test accuracy: {}'.format(test_accuracy))\n",
    "                save_path = saver.save(sess,model_path)\n",
    "                print(\"Model saved.\")\n",
    "    else:\n",
    "        # Here's where you're restoring the variables w and b.\n",
    "        # Note that the graph is exactly as it was when the variables were\n",
    "        # saved in a prior training run.\n",
    "        #ckpt = tf.train.get_checkpoint_state(check_dir)\n",
    "        #if ckpt and ckpt.model_checkpoint_path:\n",
    "        #saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "        #print('Model Restored..')\n",
    "        saver.restore(sess,model_path)\n",
    "        batch_count = int(math.ceil(n_test/batch_size))\n",
    "        total = 0\n",
    "        for i in range(batch_count):\n",
    "            batch_start = i*batch_size\n",
    "            test_batch_features = norm_test_features[batch_start:batch_start + batch_size]\n",
    "            test_batch_labels = test_labels[batch_start:batch_start + batch_size]\n",
    "            total += sess.run(acc, feed_dict={features:test_batch_features, labels: test_batch_labels, keep_prob: 1})\n",
    "        test_accuracy = total / batch_count\n",
    "        print('Restored test accuracy: {}'.format(test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "curr_path = os.getcwd()\n",
    "model_path = curr_path +'/models/model_multi.ckpt'\n",
    "\n",
    "train = True\n",
    "use_jitter = False\n",
    "steps = 100000\n",
    "batch_size = 96\n",
    "k_prob = 0.5\n",
    "learning_rate = 0.001\n",
    "# Data preparation \n",
    "if not use_jitter:\n",
    "    norm_train_features = preprocess_images(train_features)\n",
    "else:\n",
    "    norm_train_features = train_features\n",
    "    \n",
    "norm_valid_features = preprocess_images(valid_features)\n",
    "norm_test_features = preprocess_images(X_test)\n",
    "n_test = norm_test_features.shape[0]\n",
    "    \n",
    "valid_feed_dict = {features: norm_valid_features, labels : valid_labels, keep_prob: k_prob, lr_value: learning_rate}\n",
    "\n",
    "#sess = tf.Session(graph=graph_model_simple)\n",
    "#sess.run(init)\n",
    "with tf.Session(graph=graph_model_multi2) as sess:\n",
    "    if train:\n",
    "        sess.run(init)\n",
    "        for step in range(steps):\n",
    "            batch_start = np.random.choice(norm_train_features.shape[0],batch_size)\n",
    "            batch_features = norm_train_features[batch_start]\n",
    "            batch_labels = train_labels[batch_start]\n",
    "\n",
    "            if use_jitter:\n",
    "                batch_features, batch_labels = jitter_image_data(batch_features, batch_labels, batch_size)\n",
    "                # Run optimizer\n",
    "            loss_value = sess.run(train_op, feed_dict={features: batch_features, labels: batch_labels, keep_prob: k_prob, lr_value: learning_rate})\n",
    "\n",
    "            if step%1000 == 0:\n",
    "                train_accuracy = sess.run([acc], feed_dict={features:batch_features, labels: batch_labels, keep_prob: 1, lr_value: learning_rate})\n",
    "                valid_accuracy = sess.run([acc], feed_dict=valid_feed_dict)\n",
    "                batch_count = int(math.ceil(n_test/batch_size))\n",
    "                total = 0\n",
    "                for i in range(batch_count):\n",
    "                    batch_start = i*batch_size\n",
    "                    test_batch_features = norm_test_features[batch_start:batch_start + batch_size]\n",
    "                    test_batch_labels = test_labels[batch_start:batch_start + batch_size]\n",
    "                    total += sess.run(acc, feed_dict={features:test_batch_features, labels: test_batch_labels, keep_prob: 1, lr_value: learning_rate})\n",
    "                    #print(total)\n",
    "                #print(total,n_test)\n",
    "                test_accuracy = total / batch_count\n",
    "                #print(\"Steps %d, training accuracy: %g  validation accuracy: %g test accuracy: %g\"%(step, train_accuracy, valid_accuracy, test_accuracy))\n",
    "                print(\"Steps {}, training accuracy: {}  validation accuracy: {} test accuracy: {}\".format(step, train_accuracy, valid_accuracy, test_accuracy))\n",
    "                if(test_accuracy > 0.96):\n",
    "                    learning_rate = 0.0001\n",
    "\n",
    "            if ((step == (steps-1)) or (test_accuracy > 0.98)):\n",
    "                batch_count = int(math.ceil(n_test/batch_size))\n",
    "                total = 0\n",
    "                for i in range(batch_count):\n",
    "                    batch_start = i*batch_size\n",
    "                    test_batch_features = norm_test_features[batch_start:batch_start + batch_size]\n",
    "                    test_batch_labels = test_labels[batch_start:batch_start + batch_size]\n",
    "                    total += sess.run(acc, feed_dict={features:test_batch_features, labels: test_batch_labels, keep_prob: 1, lr_value:learning_rate})\n",
    "                test_accuracy = total / batch_count\n",
    "                print('Final test accuracy: {}'.format(test_accuracy))\n",
    "                save_path = saver.save(sess,model_path)\n",
    "                print(\"Model saved.\")\n",
    "    else:\n",
    "        # Here's where you're restoring the variables w and b.\n",
    "        # Note that the graph is exactly as it was when the variables were\n",
    "        # saved in a prior training run.\n",
    "        #ckpt = tf.train.get_checkpoint_state(check_dir)\n",
    "        #if ckpt and ckpt.model_checkpoint_path:\n",
    "        #saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "        #print('Model Restored..')\n",
    "        saver.restore(sess,model_path)\n",
    "        batch_count = int(math.ceil(n_test/batch_size))\n",
    "        total = 0\n",
    "        for i in range(batch_count):\n",
    "            batch_start = i*batch_size\n",
    "            test_batch_features = norm_test_features[batch_start:batch_start + batch_size]\n",
    "            test_batch_labels = test_labels[batch_start:batch_start + batch_size]\n",
    "            total += sess.run(acc, feed_dict={features:test_batch_features, labels: test_batch_labels, keep_prob: 1, lr_value: learning_rate})\n",
    "        test_accuracy = total / batch_count\n",
    "        print('Restored test accuracy: {}'.format(test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps 0, training accuracy: [0.083333336]  validation accuracy: [0.028556857] test accuracy: 0.03031179924834181\n",
      "Steps 1000, training accuracy: [0.072916672]  validation accuracy: [0.050484445] test accuracy: 0.05924698530527001\n",
      "Steps 2000, training accuracy: [0.17708334]  validation accuracy: [0.17746046] test accuracy: 0.2369704068158612\n",
      "Steps 3000, training accuracy: [0.63541669]  validation accuracy: [0.6318205] test accuracy: 0.701511669791106\n",
      "Steps 4000, training accuracy: [0.84375]  validation accuracy: [0.82304943] test accuracy: 0.8527900870099212\n",
      "Steps 5000, training accuracy: [0.92708337]  validation accuracy: [0.89699131] test accuracy: 0.9080738262696699\n",
      "Steps 6000, training accuracy: [0.91666675]  validation accuracy: [0.91432941] test accuracy: 0.924505503340201\n",
      "Steps 7000, training accuracy: [0.9375]  validation accuracy: [0.93319738] test accuracy: 0.9358165331862189\n",
      "Steps 8000, training accuracy: [0.94791669]  validation accuracy: [0.94849569] test accuracy: 0.9414019024733341\n",
      "Steps 9000, training accuracy: [0.95833337]  validation accuracy: [0.95512486] test accuracy: 0.9481271410530264\n",
      "Final test accuracy: 0.9493283864223596\n",
      "Model saved.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "curr_path = os.getcwd()\n",
    "model_path = curr_path +'/models/model_multi_jitter_small_iter.ckpt'\n",
    "\n",
    "train = True\n",
    "use_jitter = True\n",
    "steps = 10000\n",
    "batch_size = 96\n",
    "k_prob = 0.5\n",
    "learning_rate = 0.001\n",
    "# Data preparation \n",
    "if not use_jitter:\n",
    "    norm_train_features = preprocess_images(train_features)\n",
    "else:\n",
    "    norm_train_features = train_features\n",
    "    \n",
    "norm_valid_features = preprocess_images(valid_features)\n",
    "norm_test_features = preprocess_images(X_test)\n",
    "n_test = norm_test_features.shape[0]\n",
    "    \n",
    "valid_feed_dict = {features: norm_valid_features, labels : valid_labels, keep_prob: k_prob, lr_value: learning_rate}\n",
    "\n",
    "#sess = tf.Session(graph=graph_model_simple)\n",
    "#sess.run(init)\n",
    "with tf.Session(graph=graph_model_multi2) as sess:\n",
    "    if train:\n",
    "        sess.run(init)\n",
    "        for step in range(steps):\n",
    "            batch_start = np.random.choice(norm_train_features.shape[0],batch_size)\n",
    "            batch_features = norm_train_features[batch_start]\n",
    "            batch_labels = train_labels[batch_start]\n",
    "\n",
    "            if use_jitter:\n",
    "                batch_features, batch_labels = jitter_image_data(batch_features, batch_labels, batch_size)\n",
    "                # Run optimizer\n",
    "            loss_value = sess.run(train_op, feed_dict={features: batch_features, labels: batch_labels, keep_prob: k_prob, lr_value: learning_rate})\n",
    "\n",
    "            if step%1000 == 0:\n",
    "                train_accuracy = sess.run([acc], feed_dict={features:batch_features, labels: batch_labels, keep_prob: 1, lr_value: learning_rate})\n",
    "                valid_accuracy = sess.run([acc], feed_dict=valid_feed_dict)\n",
    "                batch_count = int(math.ceil(n_test/batch_size))\n",
    "                total = 0\n",
    "                for i in range(batch_count):\n",
    "                    batch_start = i*batch_size\n",
    "                    test_batch_features = norm_test_features[batch_start:batch_start + batch_size]\n",
    "                    test_batch_labels = test_labels[batch_start:batch_start + batch_size]\n",
    "                    total += sess.run(acc, feed_dict={features:test_batch_features, labels: test_batch_labels, keep_prob: 1, lr_value: learning_rate})\n",
    "                test_accuracy = total / batch_count\n",
    "                #print(\"Steps %d, training accuracy: %g  validation accuracy: %g test accuracy: %g\"%(step, train_accuracy, valid_accuracy, test_accuracy))\n",
    "                print(\"Steps {}, training accuracy: {}  validation accuracy: {} test accuracy: {}\".format(step, train_accuracy, valid_accuracy, test_accuracy))\n",
    "                if test_accuracy > 0.975:\n",
    "                    learning_rate = 0.0001\n",
    "\n",
    "            if ((step == (steps-1)) or (test_accuracy > 0.99)):\n",
    "                batch_count = int(math.ceil(n_test/batch_size))\n",
    "                total = 0\n",
    "                for i in range(batch_count):\n",
    "                    batch_start = i*batch_size\n",
    "                    test_batch_features = norm_test_features[batch_start:batch_start + batch_size]\n",
    "                    test_batch_labels = test_labels[batch_start:batch_start + batch_size]\n",
    "                    total += sess.run(acc, feed_dict={features:test_batch_features, labels: test_batch_labels, keep_prob: 1, lr_value:learning_rate})\n",
    "                test_accuracy = total / batch_count\n",
    "                print('Final test accuracy: {}'.format(test_accuracy))\n",
    "                save_path = saver.save(sess,model_path)\n",
    "                print(\"Model saved.\")\n",
    "    else:\n",
    "        # Here's where you're restoring the variables w and b.\n",
    "        # Note that the graph is exactly as it was when the variables were\n",
    "        # saved in a prior training run.\n",
    "        #ckpt = tf.train.get_checkpoint_state(check_dir)\n",
    "        #if ckpt and ckpt.model_checkpoint_path:\n",
    "        #saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "        #print('Model Restored..')\n",
    "        saver.restore(sess,model_path)\n",
    "        batch_count = int(math.ceil(n_test/batch_size))\n",
    "        total = 0\n",
    "        for i in range(batch_count):\n",
    "            batch_start = i*batch_size\n",
    "            test_batch_features = norm_test_features[batch_start:batch_start + batch_size]\n",
    "            test_batch_labels = test_labels[batch_start:batch_start + batch_size]\n",
    "            total += sess.run(acc, feed_dict={features:test_batch_features, labels: test_batch_labels, keep_prob: 1, lr_value: learning_rate})\n",
    "        test_accuracy = total / batch_count\n",
    "        print('Restored test accuracy: {}'.format(test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:sdcnd]",
   "language": "python",
   "name": "conda-env-sdcnd-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
